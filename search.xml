<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Java设计模式之代理模式]]></title>
    <url>%2F2018%2F09%2F13%2Fproxy-mode-of-java-design-pattern%2F</url>
    <content type="text"><![CDATA[定义为其他对象提供一种代理便以控制这个对象的访问。 介绍代理模式（Proxy）属于结构型模式，也叫委托模式。 代理模式的特征是代理类与委托类实现同样的接口，代理类主要负责为委托类预处理消息，过滤消息，把消息转发给委托类，以及事后消息处理。 代理类与委托类之间通常会存在关联关系，一个代理类的对象与一个委托类的对象关联，代理类的对象本身并不真正实现服务，而是通过调用委托类的对象的相关方法，来提供特定的服务。 日常生活中，代购，买房、租房都是属于常见的代理模式。 UML角色说明 Subject （抽象主题类）：接口或者抽象类，真实声明主题和代理的共同接口方法。 RealSubject（真实主题类）：被代理类或者委托类，定义代理所表示的真实对象，负责具体业务逻辑的执行，客户端可以通过代理类间接调用真实主题类的方法。 Proxy（代理类）：委托类，持有对真实主题类的引用，在其所实现接口的方法中调用真实主题类中相应的接口方法执行。 Client（客户端类）：使用代理模式的地方 模拟实现Subject类定义RealSubject和Proxy的公用接口。123456public interface Subject &#123; /** * 请求接口 */ void request();&#125; RealSubject类定义Proxy所代表的真实请求123456789public class RealSubject implements Subject &#123; /** * 请求接口 */ @Override public void request() &#123; System.out.println("我是真实的请求"); &#125;&#125; Proxy类:保存了一个引用使得代理可以访问实体，并提供一个与Subject的接口相同的接口，这种代理就可以用来替代实体；12345678910111213public class Proxy implements Subject &#123; // 需要注入bean private RealSubject realSubject; /** * 请求接口 */ @Override public void request() &#123; // 调用真实的请求 realSubject.request(); &#125;&#125; 客户端调用12345@Testpublic void test1()&#123; Proxy proxy = new Proxy(); proxy.request();&#125; 案例实现我家的娃现在喝的奶粉都是通过代购的方式从澳洲邮寄回来的，所以此处笔者就通过代购的案例来实现代理模式。 创建抽象主题类 People12345678public interface People &#123; /** * 购买 * * @param name 购买的商品名字 */ void buy(String name);&#125; 创建真实主题类假设我叫张三，需要购买奶粉。1234567891011public class Zhangsan implements People &#123; /** * 购买 * * @param name 购买的商品名字 */ @Override public void buy(String name) &#123; System.out.println("我需要代购" + name); &#125;&#125; 创建代理类，即海外代购中介123456789101112131415161718public class Purchasing implements People &#123; private Zhangsan zhangsan; public Purchasing(Zhangsan zhangsan) &#123; this.zhangsan = zhangsan; &#125; /** * 购买 * * @param name 购买的商品名字 */ @Override public void buy(String name) &#123; System.out.println("中介: 大家好，我是代购李四，很高兴为你服务。"); zhangsan.buy(name); &#125;&#125; 代购测试1234567@Testpublic void test1() &#123; // 创建代购类，并讲需要代购的人作为构造函数传递 Purchasing purchasin = new Purchasing(new Zhangsan()); // 调用代购的方法 purchasin.buy("澳洲A2奶粉 2段");&#125; 输出结果12大家好，我是代购李四，很高兴为你服务。我需要代购澳洲A2奶粉 2段 通过上面简单的代码示例，我们可以轻松理解什么是代理模式，通过代理我们可以轻松从国外购买想要的商品。 在购买商品物流过程中，如果代购害怕商品损坏，会对原有的商品在进行一次深层次的包装，避免物流暴力运输损坏商品。这个其实就是代理模式中可以增强类原本的功能。 增强功能在JAVA中常见的如通过Spring来管理Hibernate的事务，如开启事务，关闭事务。 上面简单的实现属于静态代理模式，即在程序运行前，代理类的.class文件就已经存在了。 动态代理模式所谓动态代理，就是动态代理类的.class文件是在程序运行时候由JAVA反射机制动态生成，无需提前编译。 动态代理类不仅简化了编程工作，而且提高了软件系统的可扩展性，因为Java 反射机制可以生成任意类型的动态代理类。java.lang.reflect 包中的Proxy类和InvocationHandler 接口提供了生成动态代理类的能力。 从JDK源码分析]]></content>
      <categories>
        <category>JAVA</category>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>设计模式</tag>
        <tag>代理模式</tag>
        <tag>proxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7安装MariaDB]]></title>
    <url>%2F2017%2F11%2F11%2FCentOS7-installs-MariaDB%2F</url>
    <content type="text"><![CDATA[摘要CentOS 6 或早期的版本中提供的是 MySQL 的服务器/客户端安装包，但 CentOS 7 已使用了 MariaDB 替代了默认的 MySQL。MariaDB数据库管理系统是MySQL的一个分支，主要由开源社区在维护，采用GPL授权许可 MariaDB的目的是完全兼容MySQL，包括API和命令行，使之能轻松成为MySQL的代替品。 全部删除MySQL/MariaDBMySQL 已经不再包含在 CentOS 7 的源中，而改用了 MariaDB; 删除现有软件包使用rpm -qa | grep mariadb搜索 MariaDB 现有的包： 如果存在，使用rpm -e --nodeps mariadb-*全部删除： 12345[admin@dev ~]$ rpm -qa | grep mariadbmariadb-libs-5.5.56-2.el7.x86_64[admin@dev ~]$ rpm -e mysql-*error: package mysql-* is not installed[admin@dev ~]$ 清理MySQL使用rpm -qa | grep mariadb搜索 MariaDB 现有的包： 如果存在，使用yum remove mysql mysql-server mysql-libs compat-mysql51全部删除； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[admin@dev ~]$ sudo yum remove mysql mysql-server mysql-libs compat-mysql51[sudo] password for admin:Loaded plugins: fastestmirrorNo Match for argument: mysqlNo Match for argument: mysql-serverNo Match for argument: compat-mysql51Resolving Dependencies--&gt; Running transaction check---&gt; Package mariadb-libs.x86_64 1:5.5.56-2.el7 will be erased--&gt; Processing Dependency: libmysqlclient.so.18()(64bit) for package: 2:postfix-2.10.1-6.el7.x86_64--&gt; Processing Dependency: libmysqlclient.so.18(libmysqlclient_18)(64bit) for package: 2:postfix-2.10.1-6.el7.x86_64--&gt; Running transaction check---&gt; Package postfix.x86_64 2:2.10.1-6.el7 will be erased--&gt; Finished Dependency ResolutionDependencies Resolved======================================================================================== Package Arch Version Repository Size========================================================================================Removing: mariadb-libs x86_64 1:5.5.56-2.el7 @anaconda 4.4 MRemoving for dependencies: postfix x86_64 2:2.10.1-6.el7 @anaconda 12 MTransaction Summary========================================================================================Remove 1 Package (+1 Dependent package)Installed size: 17 MIs this ok [y/N]: yDownloading packages:Running transaction checkRunning transaction testTransaction test succeededRunning transaction Erasing : 2:postfix-2.10.1-6.el7.x86_64 1/2 Erasing : 1:mariadb-libs-5.5.56-2.el7.x86_64 2/2 Verifying : 1:mariadb-libs-5.5.56-2.el7.x86_64 1/2 Verifying : 2:postfix-2.10.1-6.el7.x86_64 2/2Removed: mariadb-libs.x86_64 1:5.5.56-2.el7 Dependency Removed: postfix.x86_64 2:2.10.1-6.el7 Complete![admin@dev ~]$ rpm -qa|grep mariadb[admin@dev ~]$ 配置MariaDB开始新的安装, 创建MariaDB.repo文件 1sudo vi /etc/yum.repos.d/MariaDB.repo 新增以下内容： 1234567# MariaDB 10.2 CentOS repository list - created 2017-11-11 03:24 UTC# http://downloads.mariadb.org/mariadb/repositories/[mariadb]name = MariaDBbaseurl = http://yum.mariadb.org/10.2/centos7-amd64gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDBgpgcheck=1 系统及版本选择：https://downloads.mariadb.org/mariadb/repositories/#mirror=tuna 安装MariaDB1sudo yum -y install MariaDB-server MariaDB-client 安装成功之后启动MariaDB服务。 1234systemctl start mariadb #启动服务systemctl enable mariadb #设置开机启动systemctl restart mariadb #重新启动systemctl stop mariadb.service #停止MariaDB 登录到数据库 用mysql -uroot命令登录到MariaDB，此时root账户的密码为空。 初始化配置进行MariaDB的相关简单配置,使用mysql_secure_installation命令进行配置。1mysql_secure_installation 初始化MariaDB完成，接下来测试登录1mysql -uroot -ppassword 配置MariaDB的字符集查看/etc/my.cnf文件内容，其中包含一句!includedir /etc/my.cnf.d说明在该配置文件中引入/etc/my.cnf.d 目录下的配置文件。 使用vi server.cnf命令编辑server.cnf文件，在[mysqld]标签下添加12345init_connect=&apos;SET collation_connection = utf8_unicode_ci&apos;init_connect=&apos;SET NAMES utf8&apos;character-set-server=utf8collation-server=utf8_unicode_ciskip-character-set-client-handshake 文件/etc/my.cnf.d/mysql-clients.cnf 在[mysql]中添加 1default-character-set=utf8 全部配置完成，重启mariadb 1systemctl restart mariadb 之后进入MariaDB查看字符集 1mysql&gt; show variables like &quot;%character%&quot;;show variables like &quot;%collation%&quot;; 显示结果: 12345678910111213141516171819202122+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | /usr/share/mysql/charsets/ |+--------------------------+----------------------------+8 rows in set (0.01 sec)+----------------------+-----------------+| Variable_name | Value |+----------------------+-----------------+| collation_connection | utf8_unicode_ci || collation_database | utf8_unicode_ci || collation_server | utf8_unicode_ci |+----------------------+-----------------+3 rows in set (0.00 sec) 字符集配置完成。 添加用户，设置权限创建用户命令 12MariaDB [(none)]&gt; create user dev@localhost identified by &apos;dev123456&apos;;Query OK, 0 rows affected (0.00 sec) 授予外网登陆权限 12MariaDB [(none)]&gt; grant all privileges on *.* to dev@&apos;%&apos; identified by &apos;dev123456&apos;;Query OK, 0 rows affected (0.00 sec) 授予权限并且可以授权 1grant all privileges on *.* to dev@&apos;hostname&apos; identified by &apos;dev123456&apos; with grant option; 查看结果： 1234567891011MariaDB [mysql]&gt; select host,user,password from user;+-----------+------+-------------------------------------------+| host | user | password |+-----------+------+-------------------------------------------+| localhost | root | *84AAC12F54AB666ECFC2A83C676908C8BBC381B1 || 127.0.0.1 | root | *84AAC12F54AB666ECFC2A83C676908C8BBC381B1 || ::1 | root | *84AAC12F54AB666ECFC2A83C676908C8BBC381B1 || % | dev | *4830F76F782FF62FDDAD8358BF4F4795C98FB7DC || localhost | dev | *4830F76F782FF62FDDAD8358BF4F4795C98FB7DC || hostname | dev | *4830F76F782FF62FDDAD8358BF4F4795C98FB7DC |+-----------+------+-------------------------------------------+ 防火墙记得开放3306端口12sudo firewall-cmd --zone=public --permanent --add-port=3306/tcpservice firewalld restart]]></content>
      <categories>
        <category>数据库</category>
        <category>MariaDB</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MariaDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop环境搭建]]></title>
    <url>%2F2017%2F11%2F07%2FApache-Hadoop-environment-to-build%2F</url>
    <content type="text"><![CDATA[摘要 “工欲善其事，必先利其器”，在学习Hadoop之前，我们需要有个好的开发环境。 版本选择Apache HadoopHadoop是Apache顶级项目，我们可以直接从Apache官网下载。 官网：http://hadoop.apache.org/releases.html 目前稳定版本为 2.8.2 CDHCloidera提供Hadoop支持、咨询和管理工具的公司，在Hadoop生态圈地位举足轻重。其著名产品Cloudera’s Distribution for Hadoop，即CDH。 CDH包括Hadoop、HBase、Hive、Pig、Sqoop、Flume、Xookeeper、Oozie、Mahout等，几乎覆盖整个Hadoop生态圈，保证组件之间的兼容性。 CDH最新版本CDH5，基于Hadoop 2.3.CDH经典版本CDH3，基于Hadoop0.20.2，生产稳定版本。 笔者在学习中决定采用最新的CDH5进行研究。 下载地址：官网CDH5 Hadoop架构Hadoop主要由分布式系统HDFS和分布式计算框架MapReduce构成。 分布式文件系统主要用于海量数据的存储，MapReduce则是基于分布式文件系统对存储在分布式文件系统中的数据进行分布式计算。 Hadoop HDFS架构构成HDFS集群的主要是两类节点，并以主从（master/slave）模式，即一个NameNode和多个DataNode，还有一个节点叫SecondaryNameNode，作为NameNode镜像数据备份。 守护进程 集群数目 作用 NameNode 1 存储文件系统的元数据，存储文件与数据块映射，并提供文件系统的全景图 SecondaryNameNod 1 备份NameNode数据，并负责镜像与NameNode日志数据的合并 DataNode N个（N&gt;1） 存储块数据 MapReduce架构构成MapReduce集群为两类节点，JobTracker和TaskTracker，采用主从（master/slave）架构。 JobTracker和TaskTracker也是两种守护进程，运行在各自的节点上。客户端负责用户作业提交。 守护进程 集群数目 作用 TaskTracker 1 负责接受客户端作业提交，调度任务到TaskTracker上运行，并提供监控TaskTracker及任务进度等管理功能 TaskTracke N个（N&gt;1） 实例化用户程序，在本地执行任务并周期性地向JobTracker汇报状态。 Hadoop架构Hadoop集群部署方式。 环境准备硬件配置 操作系统 win10 64位 1709 CPU Intel I5 7500 内存 金士顿 DDR4 2400 8G 硬盘 100G以上 软件配置 虚拟机 VMWare 14 Pro 操作系统 CentOS7 JDK jdk-8u131-linux-x64.rpm 远程工具 xshell 5 Hadoop组件 hadoop-2.6.0-cdh5.13.0.tar.gzhbase-1.2.0-cdh5.13.0.tar.gzhive-1.1.0-cdh5.13.0.tar.gzpig-0.12.0-cdh5.13.0.tar.gzoozie-4.1.0-cdh5.13.0.tar.gzsqoop-1.4.6-cdh5.13.0.tar.gzmahout-0.9-cdh5.13.0.tar.gzflume-ng-1.6.0-cdh5.13.0.tar.gzzookeeper-3.4.5-cdh5.13.0.tar.gz 安装HadoopHadoop运行模式 单机模式 Hadoop默认模式，不进行任何配置，所有守护进程都是一个Java进程。 伪分布式模式 所有的守护进程都运行在一个节点上，模拟了一个具有Hadoop完整功能的微型集群。 完全分布式模式 Hadoop的守护进程运行在多个节点上，形成一个真正意义上的集群。 伪分布式模式需要1个虚拟机。完全分布式模式需要至少2台以上虚拟机。节点需要3个或者5个，保持奇数。 Hadoop安装步骤基于完全分布式模式2主3从方式 序号 IP Host 部署模块 进程 1 192.168.128.101 hadoop-master-1 NameNode、ResourceManager、JournalNode、zookeeper NameNode、ResourceManager、JournalNode、zookeeper 2 192.168.128.102 hadoop-master-2 NameNode、ResourceManager、JournalNode、zookeeper NameNode、ResourceManager、JournalNode、zookeeper 3 192.168.128.103 hadoop-slave-1 DataNode、JournalNode、zookeeper DataNode、JournalNode、zookeeper 4 192.168.128.104 hadoop-slave-2 DataNode、JournalNode DataNode、JournalNode 5 192.168.128.105 hadoop-slave-3 DataNode、JournalNode DataNode、JournalNode 安装克隆运行环境通过VMWare虚拟机克隆基础镜像centos7-base创建5台新的虚拟机，centos7-hadoop-master-1、centos7-hadoop-master-2、centos7-hadoop-slave-1、centos7-hadoop-slave-2、centos7-hadoop-slave-3。 设置主机名和用户名要查看主机名相关的设置：123hostnamectl## 或者 hostnamectl status 要同时修改所有三个主机名：静态、瞬态和灵活主机名：1hostnamectl set-hostname hadoop-master-1 新增用户名hadoop1useradd hadoop 为用户hadoop设置密码1passwd hadoop 统一密码设置为 vm2017 设置hosts，方便Hadoop各个节点之间能够通过主机名进行互相访问。1sudo vi /etc/hosts 文本后面添加12345192.168.128.101 hadoop-master-1192.168.128.102 hadoop-master-2192.168.128.103 hadoop-slave-1192.168.128.104 hadoop-slave-2192.168.128.105 hadoop-slave-3 配置静态IP地址1sudo vi /etc/sysconfig/network-scripts/ifcfg-ens33 修改IPADDR属性为对应的IP地址，记住几个虚拟机之间IP不可重复。 重启网络1service network restart 配置SSH无密码连接 关闭防火墙 123systemctl stop firewalld.service #停止firewallsystemctl disable firewalld.service #禁止firewall开机启动firewall-cmd --state #查看默认防火墙状态（关闭后显示notrunning，开启后显示running） 检查SSH是否安装 1rpm -qa |grep openssh 返回如下表示已经安装 123openssh-7.4p1-13.el7_4.x86_64openssh-server-7.4p1-13.el7_4.x86_64openssh-clients-7.4p1-13.el7_4.x86_64 1rpm -qa |grep rsync 返回如下表示已经安装 1rsync-3.0.9-18.el7.x86_64 生成SSH公钥 对于完全分布式模式，有多个节点，但是只需要主节点无密码连接从节点，因此需要在主节点生成公钥 1ssh-keygen -t rsa 遇到提示回车即可。 将公钥发至从节点的authorized_keys的列表 123ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop-slave-1ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop-slave-2ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop-slave-3 执行结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344[hadoop@hadoop-master-2 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop-slave-1/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/home/hadoop/.ssh/id_rsa.pub"The authenticity of host 'hadoop-slave-1 (192.168.128.103)' can't be established.ECDSA key fingerprint is SHA256:DChFHDTUaoTvDRo+TEzLxWWr9nkyID0813NaD/LAfew.ECDSA key fingerprint is MD5:91:bd:81:17:d3:25:7e:09:ac:8e:45:c8:f8:a3:c9:39.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keyshadoop@hadoop-slave-1's password:Number of key(s) added: 1Now try logging into the machine, with: "ssh 'hadoop@hadoop-slave-1'"and check to make sure that only the key(s) you wanted were added.[hadoop@hadoop-master-2 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop-slave-2/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/home/hadoop/.ssh/id_rsa.pub"The authenticity of host 'hadoop-slave-2 (192.168.128.104)' can't be established.ECDSA key fingerprint is SHA256:DChFHDTUaoTvDRo+TEzLxWWr9nkyID0813NaD/LAfew.ECDSA key fingerprint is MD5:91:bd:81:17:d3:25:7e:09:ac:8e:45:c8:f8:a3:c9:39.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keyshadoop@hadoop-slave-2's password:Number of key(s) added: 1Now try logging into the machine, with: "ssh 'hadoop@hadoop-slave-2'"and check to make sure that only the key(s) you wanted were added.[hadoop@hadoop-master-2 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop-slave-3/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/home/hadoop/.ssh/id_rsa.pub"The authenticity of host 'hadoop-slave-3 (192.168.128.105)' can't be established.ECDSA key fingerprint is SHA256:DChFHDTUaoTvDRo+TEzLxWWr9nkyID0813NaD/LAfew.ECDSA key fingerprint is MD5:91:bd:81:17:d3:25:7e:09:ac:8e:45:c8:f8:a3:c9:39.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keyshadoop@hadoop-slave-3's password:Number of key(s) added: 1Now try logging into the machine, with: "ssh 'hadoop@hadoop-slave-3'"and check to make sure that only the key(s) you wanted were added. 验证安装（以hadoop用户执行） 对于完全分布式模式，在主节点执行1ssh hadoop-slave-1 如果没有出现输入密码的提示则表示安装成功。12345678910111213141516[hadoop@hadoop-master-1 ~]$ ssh hadoop-slave-1Last login: Thu Nov 9 05:03:42 2017 from 192.168.128.1[hadoop@hadoop-slave-1 ~]$ exitlogoutConnection to hadoop-slave-1 closed.[hadoop@hadoop-master-1 ~]$ ssh hadoop-slave-2Last login: Thu Nov 9 05:03:54 2017 from 192.168.128.1[hadoop@hadoop-slave-2 ~]$ exitlogoutConnection to hadoop-slave-2 closed.[hadoop@hadoop-master-1 ~]$ ssh hadoop-slave-3Last login: Thu Nov 9 05:04:28 2017 from 192.168.128.1[hadoop@hadoop-slave-3 ~]$ exitlogoutConnection to hadoop-slave-3 closed.[hadoop@hadoop-master-1 ~]$ 配置Hadoop格式HDFS启动Hadoop并验证安装安装Hive]]></content>
      <categories>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>CDH5</tag>
        <tag>HDFS</tag>
        <tag>Yarn</tag>
        <tag>Flume</tag>
        <tag>MapReduce</tag>
        <tag>Hive</tag>
        <tag>Pig</tag>
        <tag>Mahout</tag>
        <tag>HBase</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop生态系统]]></title>
    <url>%2F2017%2F11%2F07%2FHadoop-past-and-present%2F</url>
    <content type="text"><![CDATA[认识HadoopHadoop是Apache开源组织的一个分布式计算开源框架http://hadoop.apache.org/,可编写和运行分布式应用处理大规模数据。 Hadoop框架的核心是HDFS和MapReduce。其中 HDFS 是分布式文件系统，MapReduce 是分布式数据处理模型和执行环境。 特点： 运行方便：Hadoop是运行在由一般商用机器构成的大型集群上。Hadoop在云计算服务层次中属于PaaS(Platform-as-a- Service)：平台即服务。 健壮性：Hadoop致力于在一般的商用硬件上运行，能够从容的处理类似硬件失效这类的故障。 可扩展性：Hadoop通过增加集群节点，可以线性地扩展以处理更大的数据集。 简单：Hadoop允许用户快速编写高效的并行代码。 生态圈Hadoop的生态系统 Nutch，互联网数据及Nutch搜索引擎应用 HDFS,Hadoop的分布式文件系统 MapReduce,分布式计算框架 Flume、Scribe，Chukwa数据收集，收集非结构化数据的工具。 Hiho、Sqoop,讲关系数据库中的数据导入HDFS的工具 Hive数据仓库，pig分析数据的工具 Oozie作业流调度引擎 Hue，Hadoop自己的监控管理工具 Avro 数据序列化工具 mahout数据挖掘工具 Hbase分布式的面向列的开源数据库 生态系统特点 源码开放 社区活跃，参与者众多 涉及分布式存储和计算的方方面面 得到业界的验证 生态组件Hadoop1.0时代的生态系统 Hadoop2.0时代的生态系统 Hadoop核心 由上图可以看出Hadoop1.0与Hadoop2.0的区别。Hadoop1.0的核心由HDFS（Hadoop Distributed File System）和MapReduce(分布式计算框架)构成。而在Hadoop2.0中增加了Yarn(Yet Another Resource Negotiator),来负责集群资源的统一管理和调度。 HDFS(分布式文件系统)HDFS源自于Google发表于2003年10月的GFS论文，也即是说HDFS是GFS的克隆版。 HDFS具有如下特点： 良好的扩展性 高容错性 适合PB级以上海量数据的存储 HDFS的基本原理 将文件切分成等大的数据块，存储到多台机器上 将数据切分、容错、负载均衡等功能透明化 可将HDFS看成容量巨大、具有高容错性的磁盘 HDFS的应用场景 海量数据的可靠性存储 数据归档 Yarn(资源管理系统)Yarn是Hadoop2.0新增的系统，负责集群的资源管理和调度，使得多种计算框架可以运行在一个集群中。 Yarn具有如下特点： 良好的扩展性、高可用性 对多种数据类型的应用程序进行统一管理和资源调度 自带了多种用户调度器，适合共享集群环境 MapReduce(分布式计算框架)MapReduce源自于Google发表于2004年12月的MapReduce论文，也就是说，Hadoop MapReduce是Google MapReduce的克隆版。 MapReduce是一种编程模型，你要函数式编程思想，将对数据集处理的过程分为Map和Reduce两个阶段。 MapReduce具有如下特点： 良好的扩展性 高容错性 适合PB级以上海量数据的离线处理 Hive(基于MR的数据仓库)Hive由facebook开源，最初用于解决海量结构化的日志数据统计问题；是一种ETL(Extraction-Transformation-Loading)工具。它也是构建在Hadoop之上的数据仓库；数据计算使用MR,数据存储使用HDFS。 Hive定义了一种类似SQL查询语言的HiveQL查询语言，除了不支持更新、索引和事务，几乎SQL的其他特征都能支持。它通常用于离线数据处理（采用MapReduce);我们可以认为Hive的HiveQL语言是MapReduce语言的翻译器，把MapReduce程序简化为HiveQL语言。但有些复杂的MapReduce程序是无法用HiveQL来描述的。 Hive提供shell、JDBC/ODBC、Thrift、Web等接口。 Hive应用场景 日志分析：统计一个网站一个时间段内的pv、uv ；比如百度。淘宝等互联网公司使用hive进行日志分析 多维度数据分析 海量结构化数据离线分析 低成本进行数据分析（不直接编写MR） Pig(数据仓库)Pig由yahoo!开源，设计动机是提供一种基于MapReduce的ad-hoc数据分析工具。它通常用于进行离线分析。 Pig是构建在Hadoop之上的数据仓库，定义了一种类似于SQL的数据流语言–Pig Latin,Pig Latin可以完成排序、过滤、求和、关联等操作，可以支持自定义函数。Pig自动把Pig Latin映射为MapReduce作业，上传到集群运行，减少用户编写Java程序的苦恼。 Pig有三种运行方式：Grunt shell、脚本方式、嵌入式。 Pig与Hive的比较 Mahout(数据挖掘库)Mahout是基于Hadoop的机器学习和数据挖掘的分布式计算框架。它实现了三大算法：推荐、聚类、分类。 HBase(分布式数据库)HBase源自Google发表于2006年11月的Bigtable论文。也就是说，HBase是Google Bigtable的克隆版。 HBase可以使用shell、web、api等多种方式访问。它是NoSQL的典型代表产品。 HBase的特点 高可靠性 高性能 面向列 良好的扩展性 HBase的数据模型 Table（表）：类似于传统数据库中的表 Column Family(列簇)：Table在水平方向有一个或者多个Column Family组成；一个Column Family 中可以由任意多个Column组成。 Row Key(行健)：Table的主键；Table中的记录按照Row Key排序。 Timestamp（时间戳）：每一行数据均对应一个时间戳；也可以当做版本号。 Zookeeper(分布式协作服务)Zookeeper源自Google发表于2006年11月的Chubby论文，也就是说Zookeeper是Chubby的克隆版。 Zookeeper解决分布式环境下数据管理问题： 统一命名 状态同步 集群管理 配置同步 Zookeeper的应用 HDFS Yarn Storm HBase Flume Dubbo Metaq Sqoop（数据同步工具）Sqoop是连接Hadoop与传统数据库之间的桥梁，它支持多种数据库，包括MySQL、DB2等；插拔式，用户可以根据需要支持新的数据库。 Sqoop实质上是一个MapReduce程序，充分利用MR并行的特点,充分利用MR的容错性。 Flume(日志收集工具)Flume是Cloudera提供的一个高可用、高可靠、分布式的海量日志采集、聚合和传输系统，Flume支持在日志系统中定制各类数据发送方，用于手机数据。 Flume的特点 分布式 高可靠性 高容错性 易于定制与扩展 Oozie(作业流调度系统)目前计算框架和作业类型种类繁多：如MapReduce、Stream、HQL、Pig等。这些作业之间存在依赖关系，周期性作业，定时执行的作业，作业执行状态监控与报警等。如何对这些框架和作业进行统一管理和调度？ 解决方案有多种： Linux Crontab 自己设计调度系统（淘宝等公司） 直接使用开源系统（Oozie） Kafka(消息队列)高吞吐量的分布式分部订阅消息系统。]]></content>
      <categories>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
        <tag>Yarn</tag>
        <tag>Flume</tag>
        <tag>MapReduce</tag>
        <tag>Hive</tag>
        <tag>Pig</tag>
        <tag>Mahout</tag>
        <tag>HBase</tag>
        <tag>Zookeeper</tag>
        <tag>生态系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务实践（七）：从单体式架构迁移到微服务架构]]></title>
    <url>%2F2017%2F11%2F05%2FRefactoring-a-Monolith-into-Microservices%2F</url>
    <content type="text"><![CDATA[摘要这是用微服务开发应用系列博客的第七篇也是最后一篇。第一篇中介绍了微服务架构模式，并且讨论了微服架构的优缺点；接续文章讨论了微服务架构不同方面：使用API网关，进程间通信，服务发现，事件驱动数据管理以及部署微服务。本篇，我们将探讨将应用从单体式架构迁移到微服务架构需要考虑的策略。 迁移到微服务综述迁移单体式应用到微服务架构意味着一系列现代化过程，有点像这几代开发者一直在做的事情，实时上，当迁移时，我们可以重用一些想法。 一个策略是：不要大规模（big bang）重写代码（只有当你承担重建一套全新基于微服务的应用时候可以采用重写这种方法）。重写代码听起来很不错，但实际上充满了风险最终可能会失败，就如Martin Fowler所说：“the only thing a Big Bang rewrite guarantees is a Big Bang!” 相反，应该采取逐步迁移单体式应用的策略，通过逐步生成微服务新应用，与旧的单体式应用集成，随着时间推移，单体式应用在整个架构中比例逐渐下降直到消失或者成为微服务架构一部分。这个策略有点像在高速路上限速到70迈对车做维护，尽管有挑战，但是比起重写的风险小很多。 Martin Fowler将这种现代化策略成为绞杀（Strangler）应用，名字来源于雨林中的绞杀藤（strangler vine），也叫绞杀榕(strangler fig)。绞杀藤为了爬到森林顶端都要缠绕着大叔生长，一段时间后，树死了，留下树形藤。这种应用也使用同一种模式，围绕着传统应用开发了新型微服务应用，传统应用会渐渐退出舞台。 我们来看看其他可行策略。 策略1——停止挖掘Law of Holes是说当自己进洞就应该停止挖掘。对于单体式应用不可管理时这是最佳建议。换句话说，应该停止让单体式应用继续变大，也就是说当开发新功能时不应该为旧单体应用添加新代码，最佳方法应该是将新功能开发成独立微服务。如下图所示： 除了新服务和传统应用，还有两个模块，其一是请求路由器，负责处理入口（http）请求，有点像之前提到的API网关。路由器将新功能请求发送给新开发的服务，而将传统请求还发给单体式应用。 另外一个是胶水代码（glue code），将微服务和单体应用集成起来，微服务很少能独立存在，经常会访问单体应用的数据。胶水代码，可能在单体应用或者为服务或者二者兼而有之，负责数据整合。微服务通过胶水代码从单体应用中读写数据。​ 微服务有三种方式访问单体应用数据： 换气单体应用提供的远程API 直接访问单体应用数据库 自己维护一份从单体应用中同步的数据 胶水代码也被称为容灾层（anti-corruption layer），这是因为胶水代码保护微服务全新域模型免受传统单体应用域模型污染。胶水代码在这两种模型间提供翻译功能。术语anti-corruption layer第一次出现在Eric Evans撰写的必读书Domain Driven Design，随后就被提炼为一篇白皮书。开发容灾层可能有点不是很重要，但却是避免单体式泥潭的必要部分。 将新功能以轻量级微服务方式实现由很多优点，例如可以阻止单体应用变的更加无法管理。微服务本身可以开发、部署和独立扩展。采用微服务架构会给开发者带来不同的切身感受。 然而，这方法并不解决任何单体式本身问题，为了解决单体式本身问题必须深入单体应用​做出改变。我们来看看这么做的策略。 策略2——将前端和后端分离 减小单体式应用复杂度的策略是讲表现层和业务逻辑、数据访问层分开。典型的企业应用至少有三个不同元素构成： 表现层——处理HTTP请求，要么响应一个RESTAPI请求，要么是提供一个基于HTML的图形接口。对于一个复杂用户接口应用，表现层经常是代码重要的部分。 业务逻辑层——完成业务逻辑的应用核心​ 数据访问层——访问基础元素，例如数据库和消息代理​ 在表现层与业务数据访问层之间有清晰的隔离。业务层有由若干方面组成的粗粒度（coarse-grained）的API，内部包含了业务逻辑元素。API是可以将单体业务分割成两个更小应用的天然边界，其中一个应用是表现层，另外一个是业务和数据访问逻辑。分割后，表现逻辑应用远程调用业务逻辑应用，下图表示迁移前后架构不同：​ 单体应用这么分割有两个好处，其一使得应用两部分开发、部署和扩展各自独立，特别地，允许表现层开发者在用户界面上快速选择，进行A/B测试；其二，使得一些远程API可以被微服务调用。 然而，这种策略只是部分的解决方案。很可能应用的两部分之一或者全部都是不可管理的，因此需要使用第三种策略来消除剩余的单体架构。 策略3——抽出服务第三种迁移策略就是从单体应用中抽取出某些模块成为独立微服务。每当抽取一个模块变成微服务，单体应用就变简单一些；一旦转换足够多的模块，单体应用本身已经不成为问题了，要么消失了，要么简单到成为一个服务。 排序那个模块应该被转成微服务一个巨大的复杂单体应用由成十上百个模块构成，每个都是被抽取对象。决定第一个被抽取模块一般都是挑战，一般最好是从最容易抽取的模块开始，这会让开发者积累足够经验，这些经验可以为后续模块化工作带来巨大好处。 转换模块成为微服务一般很耗费时间，一般可以根据获益程度来排序，一般从经常变化模块开始会获益最大。一旦转换一个模块为微服务，就可以将其开发部署成独立模块，从而加速开发进程。 将资源消耗大户先抽取出来也是排序标准之一。例如，将内存数据库抽取出来成为一个微服务会非常有用，可以将其部署在大内存主机上。同样的，将对计算资源很敏感的算法应用抽取出来也是非常有益的，这种服务可以被部署在有很多CPU的主机上。通过将资源消耗模块转换成微服务，可以使得应用易于扩展。 查找现有粗粒度边界来决定哪个模块应该被抽取，也是很有益的，这使得移植工作更容易和简单。例如，只与其他应用异步同步消息的模块就是一个明显边界，可以很简单容易地将其转换为微服务。 如何抽取模块抽取模块第一步就是定义好模块和单体应用之间粗粒度接口，由于单体应用需要微服务的数据，反之亦然，因此更像是一个双向API。因为必须在负责依赖关系和细粒度接口模式之间做好平衡，因此开发这种API很有挑战性，尤其对使用域模型模式的业务逻辑层来说更具有挑战，因此经常需要改变代码来解决依赖性问题，如图所示： ​一旦完成粗粒度接口，也就将此模块转换成独立微服务。为了实现，必须写代码使得单体应用和微服务之间通过使用进程间通信（IPC）机制的API来交换信息。如图所示迁移前后对比： 此例中，正在使用Y模块的Z模块是备选抽取模块，其元素正在被X模块使用，迁移第一步就是定义一套粗粒度APIs，第一个接口应该是被X模块使用的内部接口，用于激活Z模块；第二个接口是被Z模块使用的外部接口，用于激活Y模块。 迁移第二步就是将模块转换成独立服务。内部和外部接口都使用基于IPC机制的代码，一般都会将Z模块整合成一个微服务基础框架，来出来割接过程中的问题，例如服务发现。 抽取完模块，也就可以开发、部署和扩展另外一个服务，此服务独立于单体应用和其它服务。可以从头写代码实现服务；这种情况下，将服务和单体应用整合的API代码成为容灾层，在两种域模型之间进行翻译工作。每抽取一个服务，就朝着微服务方向前进一步。随着时间推移，单体应用将会越来越简单，用户就可以增加更多独立的微服务。 总结将现有应用迁移成微服务架构的现代化应用，不应该通过从头重写代码方式实现，相反，应该通过逐步迁移的方式。有三种策略可以考虑：将新功能以微服务方式实现；将表现层与业务数据访问层分离；将现存模块抽取变成微服务。随着时间推移，微服务数量会增加，开发团队的弹性和效率将会大大增加。 原文链接：Refactoring a Monolith into Microservices（翻译：杨峰） 本系列七篇文章列表如下： 微服务实战（一）：微服务架构的优势与不足 微服务实战（二）：使用API Gateway 微服务实战（三）：深入微服务架构的进程间通信 微服务实战（四）：服务发现的可行方案以及实践案例 微服务实践（五）：微服务的事件驱动数据管理 微服务实践（六）：选择微服务部署策略 微服务实践（七）：从单体式架构迁移到微服务架构(本文)]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务实践（六）：选择微服务部署策略]]></title>
    <url>%2F2017%2F11%2F05%2FChoosing-a-Microservices-Deployment-Strategy%2F</url>
    <content type="text"><![CDATA[这篇博客是用微服务建应用的第六篇，第一篇介绍了微服务架构模板，并且讨论了使用微服务的优缺点。随后的文章讨论了微服务不同方面：使用API网关，进程间通讯，服务发现和事件驱动数据管理。这篇文章，我们将讨论部署微服务的策略。 动机部署一个单体式应用意味运行大型应用的多个副本，典型的提供若干个（N）服务器（物理或者虚拟），运行若干个（M）个应用实例。部署单体式应用不会很直接，但是肯定比部署微服务应用简单些。 一个微服务应用由上百个服务构成，服务可以采用不同语言和框架分别写就。每个服务都是一个单一应用，可以有自己的部署、资源、扩展和监控需求。例如，可以根据服务需求运行若干个服务实例，除此之外，每个实例必须有自己的CPU，内存和I/O资源。尽管很复杂，但是更挑战的是服务部署必须快速、可靠和性价比高。 有一些微服务部署的模式，先讨论一下每个主机多服务实例的模式。 单主机多服务实例模式部署微服务的一种方法就是单主机多服务实例模式，使用这种模式，需要提供若干台物理或者虚拟机，每台机器上运行多个服务实例。很多情况下，这是传统的应用部署方法。每个服务实例运行一个或者多个主机的well-known端口，主机可以看做宠物。 下图展示的是这种架构： 这种模式有一些参数，一个参数代表每个服务实例由多少进程构成。例如，需要在Apache Tomcat Server上部署一个Java服务实例作为web应用。一个Node.js服务实例可能有一个父进程和若干个子进程构成。 另外一个参数定义同一进程组内有多少服务实例运行。例如，可以在同一个Apache Tomcat Server上运行多个Java web应用，或者在同一个OSGI容器内运行多个OSGI捆绑实例。 单主机多服务实例模式也是优缺点并存。主要优点在于资源利用有效性。多服务实例共享服务器和操作系统，如果进程组运行多个服务实例效率会更高，例如，多个web应用共享同一个Apache Tomcat Server和JVM。 另一个优点在于部署服务实例很快。只需将服务拷贝到主机并启动它。如果服务用Java写的，只需要拷贝JAR或者WAR文件即可。对于其它语言，例如Node.js或者Ruby，需要拷贝源码。也就是说网络负载很低。 因为没有太多负载，启动服务很快。如果服务是自包含的进程，只需要启动就可以；否则，如果是运行在容器进程组中的某个服务实例，则需要动态部署进容器中，或者重启容器。 除了上述优点外，单主机多服务实例也有缺陷。其中一个主要缺点是服务实例间很少或者没有隔离，除非每个服务实例是独立进程。如果想精确监控每个服务实例资源使用，就不能限制每个实例资源使用。因此有可能造成某个糟糕的服务实例占用了主机的所有内存或者CPU。 同一进程内多服务实例没有隔离。所有实例有可能，例如，共享同一个JVM heap。某个糟糕服务实例很容易攻击同一进程中其它服务；更甚至于，有可能无法监控每个服务实例使用的资源情况。 另一个严重问题在于运维团队必须知道如何部署的详细步骤。服务可以用不同语言和框架写成，因此开发团队肯定有很多需要跟运维团队沟通事项。其中复杂性增加了部署过程中出错的可能性。 可以看到，尽管熟悉，但是单主机多服务实例有很多严重缺陷。下面看看是否有其他部署微服务方式能够避免这些问题。 单主机单服务实例模式另外一种部署微服务方式是单主机单实例模式。当使用这种模式，每个主机上服务实例都是各自独立的。有两种不同实现模式：单虚拟机单实例和单容器单实例。 单虚拟机单实例模式但是用单虚拟机单实例模式，一般将服务打包成虚拟机映像（image），例如一个Amazon EC2 AMI。每个服务实例是一个使用此映像启动的VM（例如，EC2实例）。下图展示了此架构： Netfix采用这种架构部署video streaming service。Netfix使用Aminator将每个服务打包成一个EC2 AMI。每个运行服务实例就是一个EC2实例。 有很多工具可以用来搭建自己的VMs。可以配置持续集成（CI）服务（例如，Jenkins）避免Aminator将服务打包成EC2 AMI。packer.io是自动虚机映像创建的另外一种选择。跟Aminator不同，它支持一系列虚拟化技术，例如EC2，DigitalOcean，VirtualBox和VMware。​ Boxfuse公司有一个创新方法创建虚机映像，克服了如下缺陷。Boxfuse将java应用打包成最小虚机映像，它们创建迅速，启动很快，因为对外暴露服务接口少而更加安全。 CloudNative公司有一个用于创建EC2 AMI的SaaS应用，Bakery。用户微服务架构通过测试后，可以配置自己的CI服务器激活Bakery。Bakery将服务打包成AMI。使用如Bakery的SaaS应用意味着用户不需要浪费时间在设置自己的AMI创建架构。 每虚拟机服务实例模式有许多优势，主要的VM优势在于每个服务实例都是完全独立运行的，都有各自独立的CPU和内存而不会被其它服务占用。 另外一个好处在于用户可以使用成熟云架构，例如AWS提供的，云服务都提供如负载均衡和扩展性等有用功能。 还有一个好处在于服务实施技术被自包含了。一旦服务被打包成VM就成为一个黑盒子。VM的管理API成为部署服务的API，部署成为一个非常简单和可靠的事情。 单虚拟机单实例模式也有缺点。一个缺点就是资源利用效率不高。每个服务实例战友整个虚机的资源，包括操作系统。而且，在一个典型的公有IaaS环境，虚机资源都是标准化的，有可能未被充分利用。 而且，公有IaaS根据VM来收费，而不管虚机是否繁忙；例如AWS提供了自动扩展功能，但是对随需应用缺乏快速响应，使得用户不得不多部署虚机，从而增加了部署费用。 另外一个缺点在于部署服务新版本比较慢。虚机镜像因为大小原因创建起来比较慢，同样原因，虚机初始化也比较慢，操作系统启动也需要时间。但是这并不一直是这样，一些轻量级虚机，例如使用Boxfuse创建的虚机，就比较快。 第三个缺点是对于运维团队，它们负责许多客制化工作。除非使用如Boxfuse之类的工具，可以帮助减轻大量创建和管理虚机的工作；否则会占用大量时间从事与核心业务不太无关的工作。 那么我们来看看另外一种仍然具有虚机特性，但是比较轻量的微服务部署方法。 单容器单服务实例模式当使用这种模式时，每个服务实例都运行在各自容器中。容器是运行在操作系统层面的虚拟化机制。一个容器包含若干运行在沙箱中的进程。从进程角度来看，他们有各自的命名空间和根文件系统；可以限制容器的内存和CPU资源。某些容器还具有I/O限制，这类容器技术包括Docker和Solaris Zones。 下图展示了这种模式： ​使用这种模式需要将服务打包成容器映像。一个容器映像是一个运行包含服务所需库和应用的文件系统​。某些容器映像由完整的linux根文件系统组成，其它则是轻量级的。例如，为了部署Java服务，需要创建包含Java运行库的容器映像，也许还要包含Apache Tomcat server，以及编译过的Java应用。 一旦将服务打包成容器映像，就需要启动若干容器。一般在一个物理机或者虚拟机上运行多个容器，可能需要集群管理系统，例如k8s或者Marathon，来管理容器。集群管理系统将主机作为资源池，根据每个容器对资源的需求，决定将容器调度到那个主机上。 单容器单服务实例模式也是优缺点都有。容器的优点跟虚机很相似，服务实例之间完全独立，可以很容易监控每个容器消耗的资源。跟虚机相似，容器使用隔离技术部署服务。容器管理API也可以作为管理服务的API。 然而，跟虚机不一样，容器是一个轻量级技术。容器映像创建起来很快，例如，在笔记本电脑上，将Spring Boot 应用打包成容器映像只需要5秒钟。因为不需要操作系统启动机制，容器启动也很快。当容器启动时，后台服务就启动了。 使用容器也有一些缺点。尽管容器架构发展迅速，但是还是不如虚机架构成熟。而且由于容器之间共享host OS内核因此并不像虚机那么安全。 另外，容器技术将会对管理容器映像提出许多客制化需求，除非使用如Google Container Engine或者Amazon EC2 Container Service (ECS)，否则用户将同时需要管理容器架构以及虚机架构。 第三，容器经常被部署在按照虚机收费的架构上，很显然，客户也会增加部署费用来应对负载的增长。 有趣的是，容器和虚机之间的区别越来越模糊。如前所述，Boxfuse虚机启动创建都很快，Clear Container技术面向创建轻量级虚机。unikernel公司的技术也引起大家关注，Docker最近收购了Unikernel公司。 除了这些之外，server-less部署技术，避免了前述容器和VM技术的缺陷，吸引了越来越多的注意。下面我们来看看。 Serverless 部署AWS Lambda是serverless部署技术的例子，支持Java，Node.js和Python服务；需要将服务打包成ZIP文件上载到AWS Lambda就可以部署。可以提供元数据，提供处理服务请求函数的名字（一个事件）。AWS Lambda自动运行处理请求足够多的微服务，然而只根据运行时间和消耗内存量来计费。当然细节决定成败，AWS Lambda也有限制。但是大家都不需要担心服务器，虚拟机或者容器内的任何方面绝对吸引人。 Lambda 函数 是无状态服务。一般通过激活AWS服务处理请求。例如，当映像上载到S3 bucket激活Lambda函数后，就可以在DynamoDB映像表中插入一个条目，给Kinesis流发布一条消息，触发映像处理动作。Lambda函数也可以通过第三方web服务激活。 有四种方法激活Lambda函数： 直接方式，使用web服务请求 自动方式，回应例如AWS S3，DynamoDB，Kinesis或者Simple Email Service等产生的事件 自动方式，通过AWS API网关来处理应用客户端发出的HTTP请求​ 定时方式，通过cron响应​–很像定时器方式 可以看出，AWS Lambda是一种很方便部署微服务的方式。基于请求计费方式意味着用户只需要承担处理自己业务那部分的负载；另外，因为不需要了解基础架构，用户只需要开发自己的应用。 然而还是有不少限制。不需要用来部署长期服务，例如用来消费从第三方代理转发来的消息，请求必须在300秒内完成，服务必须是无状态，因为理论上AWS Lambda会为每个请求生成一个独立的实例；必须用某种支持的语言完成，服务必须启动很快，否则，会因为超时被停止。 总结部署微服务应用也是一种挑战。用各种语言和框架写成的服务成百上千。每种服务都是一种迷你应用，有自己独特的部署、资源、扩充和监控需求。有若干种微服务部署模式，包括单虚机单实例以及单容器单实例。另外可选模式还有AWS Lambda，一种serverless方法。 原文链接：Choosing a Microservices Deployment Strategy（翻译：杨峰） 本系列七篇文章列表如下： 微服务实战（一）：微服务架构的优势与不足 微服务实战（二）：使用API Gateway 微服务实战（三）：深入微服务架构的进程间通信 微服务实战（四）：服务发现的可行方案以及实践案例 微服务实践（五）：微服务的事件驱动数据管理 微服务实践（六）：选择微服务部署策略(本文) 微服务实践（七）：从单体式架构迁移到微服务架构]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务实践（五）：微服务的事件驱动数据管理]]></title>
    <url>%2F2017%2F11%2F05%2FEvent-Driven-Data-Management-for-Microservices%2F</url>
    <content type="text"><![CDATA[本文是使用微服务创建应用系列的第五篇文章。第一篇文章介绍了微服务架构模式，并且讨论了使用微服务的优缺点；第二和第三篇描述了微服务架构模块间通讯的不同方面；第四篇研究了服务发现中的问题。本篇中，我们从另外一个角度研究一下微服务架构带来的分布式数据管理问题。 1.1 微服务和分布式数据管理问题单体式应用一般都会有一个关系型数据库，由此带来的好处是应用可以使用 ACID transactions，可以带来一些重要的操作特性： 原子性 – 任何改变都是原子性的 一致性 – 数据库状态一直是一致性的 隔离性 – 即使交易并发执行，看起来也是串行的 Durable – 一旦交易提交了就不可回滚 鉴于以上特性，应用可以简化为：开始一个交易，改变（插入，删除，更新）很多行，然后提交这些交易。 使用关系型数据库带来另外一个优势在于提供SQL（功能强大，可声明的，表转化的查询语言）支持。用户可以非常容易通过查询将多个表的数据组合起来，RDBMS查询调度器决定最佳实现方式，用户不需要担心例如如何访问数据库等底层问题。另外，因为所有应用的数据都在一个数据库中，很容易去查询。 然而，对于微服务架构来说，数据访问变得非常复杂，这是因为数据都是微服务私有的，唯一可访问的方式就是通过API。这种打包数据访问方式使得微服务之间松耦合，并且彼此之间独立。如果多个服务访问同一个数据，schema会更新访问时间，并在所有服务之间进行协调。 更甚于，不同的微服务经常使用不同的数据库。应用会产生各种不同数据，关系型数据库并不一定是最佳选择。某些场景，某个NoSQL数据库可能提供更方便的数据模型，提供更加的性能和可扩展性。例如，某个产生和查询字符串的应用采用例如Elasticsearch的字符搜索引擎。同样的，某个产生社交图片数据的应用可以采用图片数据库，例如，Neo4j；因此，基于微服务的应用一般都使用SQL和NoSQL结合的数据库，也就是被称为polyglot persistence的方法。 分区的，polyglot-persistent架构用于存储数据有许多优势，包括松耦合服务和更佳性能和可扩展性。然而，随之而来的则是分布式数据管理带来的挑战。 第一个挑战在于如何完成一笔交易的同时保持多个服务之间数据一致性。之所以会有这个问题，我们以一个在线B2B商店为例，客户服务维护包括客户的各种信息，例如credit lines。订单服务管理订单，需要验证某个新订单与客户的信用限制没有冲突。在单一式应用中，订单服务只需要使用ACID交易就可以检查可用信用和创建订单。 相反的，微服务架构下，订单和客户表分别是相对应服务的私有表，如下图所示： 订单服务不能直接访问客户表，只能通过客户服务发布的API来访问。订单服务也可以使用 distributed transactions, 也就是周知的两阶段提交 (2PC)。然而，2PC在现在应用中不是可选性。根据CAP理论，必须在可用性（availability）和ACID一致性（consistency）之间做出选择，availability一般是更好的选择。但是，许多现代科技，例如许多NoSQL数据库，并不支持2PC。在服务和数据库之间维护数据一致性是非常根本的需求，因此我们需要找其他的方案。 第二个挑战是如何完成从多个服务中搜索数据。例如，设想应用需要显示客户和他的订单。如果订单服务提供API来接受用户订单信息，那么用户可以使用类应用型的join操作接收数据。应用从用户服务接受用户信息，从订单服务接受此用户订单。假设，订单服务只支持通过私有键（key）来查询订单（也许是在使用只支持基于主键接受的NoSQL数据库），此时，没有合适的方法来接收所需数据。 1.2 事件驱动架构对许多应用来说，这个解决方案就是使用事件驱动架构（event-driven architecture）。在这种架构中，当某件重要事情发生时，微服务会发布一个事件，例如更新一个业务实体。当订阅这些事件的微服务接收此事件时，就可以更新自己的业务实体，也可能会引发更多的时间发布。 可以使用事件来实现跨多服务的业务交易。交易一般由一系列步骤构成，每一步骤都由一个更新业务实体的微服务和发布激活下一步骤的事件构成。下图展现如何使用事件驱动方法，在创建订单时检查信用可用度，微服务通过消息代理（Messsage Broker）来交换事件。 订单服务创建一个带有NEW状态的Order （订单），发布了一个“Order Created Event（创建订单）”的事件。 客户服务消费Order Created Event事件，为此订单预留信用，发布“Credit Reserved Event（信用预留）”事件 订单服务消费Credit Reserved Event，改变订单的状态为OPEN 更复杂的场景可以引入更多步骤，例如在检查用户信用的同时预留库存等。 考虑到（a）每个服务原子性更新数据库和发布事件，然后，（b）消息代理确保事件传递至少一次，然后可以跨多个服务完成业务交易（此交易不是ACID交易）。这种模式提供弱确定性，例如最终一致性 eventual consistency。这种交易类型被称作 BASE model。 亦可以使用事件来维护不同微服务拥有数据预连接（pre-join）的实现视图。维护此视图的服务订阅相关事件并且更新视图。例如，客户订单视图更新服务（维护客户订单视图）会订阅由客户服务和订单服务发布的事件。 当客户订单视图更新服务收到客户或者订单事件，就会更新 客户订单视图数据集。可以使用文档数据库（例如MongoDB）来实现客户订单视图，为每个用户存储一个文档。客户订单视图查询服务负责响应对客户以及最近订单（通过查询客户订单视图数据集）的查询。 事件驱动架构也是既有优点也有缺点，此架构可以使得交易跨多个服务且提供最终一致性，并且可以使应用维护最终视图；而缺点在于编程模式比ACID交易模式更加复杂：为了从应用层级失效中恢复，还需要完成补偿性交易，例如，如果信用检查不成功则必须取消订单；另外，应用必须应对不一致的数据，这是因为临时（in-flight）交易造成的改变是可见的，另外当应用读取未更新的最终视图时也会遇见数据不一致问题。另外一个缺点在于订阅者必须检测和忽略冗余事件。 1.3 原子操作Achieving Atomicity事件驱动架构还会碰到数据库更新和发布事件原子性问题。例如，订单服务必须向ORDER表插入一行，然后发布Order Created event，这两个操作需要原子性。如果更新数据库后，服务瘫了（crashes）造成事件未能发布，系统变成不一致状态。确保原子操作的标准方式是使用一个分布式交易，其中包括数据库和消息代理。然而，基于以上描述的CAP理论，这却并不是我们想要的。 1.3.1 使用本地交易发布事件获得原子性的一个方法是对发布事件应用采用multi-step process involving only local transactions，技巧在于一个EVENT表，此表在存储业务实体数据库中起到消息列表功能。应用发起一个（本地）数据库交易，更新业务实体状态，向EVENT表中插入一个事件，然后提交此次交易。另外一个独立应用进程或者线程查询此EVENT表，向消息代理发布事件，然后使用本地交易标志此事件为已发布，如下图所示： 订单服务向ORDER表插入一行，然后向EVENT表中插入Order Created event，事件发布线程或者进程查询EVENT表，请求未发布事件，发布他们，然后更新EVENT表标志此事件为已发布。 此方法也是优缺点都有。优点是可以确保事件发布不依赖于2PC，应用发布业务层级事件而不需要推断他们发生了什么；而缺点在于此方法由于开发人员必须牢记发布事件，因此有可能出现错误。另外此方法对于某些使用NoSQL数据库的应用是个挑战，因为NoSQL本身交易和查询能力有限。 此方法因为应用采用了本地交易更新状态和发布事件而不需要2PC，现在再看看另外一种应用简单更新状态获得原子性的方法。 1.3.2 挖掘数据库交易日志另外一种不需要2PC而获得线程或者进程发布事件原子性的方式就是挖掘数据库交易或者提交日志。应用更新数据库，在数据库交易日志中产生变化，交易日志挖掘进程或者线程读这些交易日志，将日志发布给消息代理。如下图所见： 此方法的例子如LinkedIn Databus 项目，Databus 挖掘Oracle交易日志，根据变化发布事件，LinkedIn使用Databus来保证系统内各记录之间的一致性。 另外的例子如：AWS的 streams mechanism in AWS DynamoDB，是一个可管理的NoSQL数据库，一个DynamoDB流是由过去24小时对数据库表基于时序的变化（创建，更新和删除操作），应用可以从流中读取这些变化，然后以事件方式发布这些变化。 交易日志挖掘也是优缺点并存。优点是确保每次更新发布事件不依赖于2PC。交易日志挖掘可以通过将发布事件和应用业务逻辑分离开得到简化；而主要缺点在于交易日志对不同数据库有不同格式，甚至不同数据库版本也有不同格式；而且很难从底层交易日志更新记录转换为高层业务事件。 交易日志挖掘方法通过应用直接更新数据库而不需要2PC介入。下面我们再看一种完全不同的方法：不需要更新只依赖事件的方法。 1.3.3 使用事件源Event sourcing （事件源）通过使用根本不同的事件中心方式来获得不需2PC的原子性，保证业务实体的一致性。 这种应用保存业务实体一系列状态改变事件，而不是存储实体现在的状态。应用可以通过重放事件来重建实体现在状态。只要业务实体发生变化，新事件就会添加到时间表中。因为保存事件是单一操作，因此肯定是原子性的。 为了理解事件源工作方式，考虑事件实体作为一个例子。传统方式中，每个订单映射为ORDER表中一行，例如在ORDER_LINE_ITEM表中。但是对于事件源方式，订单服务以事件状态改变方式存储一个订单：创建的，已批准的，已发货的，取消的；每个事件包括足够数据来重建订单状态。 事件是长期保存在事件数据库中，提供API添加和获取实体事件。事件存储跟之前描述的消息代理类似，提供API来订阅事件。事件存储将事件递送到所有感兴趣的订阅者，事件存储是事件驱动微服务架构的基干。 事件源方法有很多优点：解决了事件驱动架构关键问题，使得只要有状态变化就可以可靠地发布事件，也就解决了微服务架构中数据一致性问题。另外，因为是持久化事件而不是对象，也就避免了object relational impedance mismatch problem。 数据源方法提供了100%可靠的业务实体变化监控日志，使得获取任何时点实体状态成为可能。另外，事件源方法可以使得业务逻辑可以由事件交换的松耦合业务实体构成。这些优势使得单体应用移植到微服务架构变的相对容易。 事件源方法也有不少缺点，因为采用不同或者不太熟悉的变成模式，使得重新学习不太容易；事件存储只支持主键查询业务实体，必须使用 Command Query Responsibility Segregation (CQRS) 来完成查询业务，因此，应用必须处理最终一致数据。 1.4 总结在微服务架构中，每个微服务都有自己私有的数据集。不同微服务可能使用不同的SQL或者NoSQL数据库。尽管数据库架构有很强的优势，但是也面对数据分布式管理的挑战。第一个挑战就是如何在多服务之间维护业务交易一致性；第二个挑战是如何从多服务环境中获取一致性数据。 最佳解决办法是采用事件驱动架构。其中碰到的一个挑战是如何原子性的更新状态和发布事件。有几种方法可以解决此问题，包括将数据库视为消息队列、交易日志挖掘和事件源。 原文链接：Event-Driven Data Management for Microservices（翻译：杨峰） 本系列七篇文章列表如下： 微服务实战（一）：微服务架构的优势与不足 微服务实战（二）：使用API Gateway 微服务实战（三）：深入微服务架构的进程间通信 微服务实战（四）：服务发现的可行方案以及实践案例 微服务实践（五）：微服务的事件驱动数据管理(本文) 微服务实践（六）：选择微服务部署策略 微服务实践（七）：从单体式架构迁移到微服务架构]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务实战（四）：服务发现的可行方案以及实践案例]]></title>
    <url>%2F2017%2F11%2F05%2FService-Discovery-in-a-Microservices-Architecture%2F</url>
    <content type="text"><![CDATA[这是关于使用微服务架构创建应用系列的第四篇文章。第一篇介绍了微服务架构的模式，讨论了使用微服务架构的优缺点。第二和第三篇描述了微服务架构内部的通讯机制。这篇文章中，我们将会探讨服务发现相关问题。 为什么要使用服务发现?设想一下，我们正在写代码使用了提供REST API或者Thrift API的服务，为了完成一次服务请求，代码需要知道服务实例的网络位置（IP地址和端口）。传统应用都运行在物理硬件上，服务实例的网络位置都是相对固定的。例如，代码可以从一个经常变更的配置文件中读取网络位置。 而对于一个现代的，基于云微服务的应用来说，这却是一个很麻烦的问题。其架构如图所示： 服务实例的网络位置都是动态分配的，而且因为扩展、失效和升级等需求，服务实例会经常动态改变，因此，客户端代码需要使用一种更加复杂的服务发现机制。 目前有两大类服务发现模式：客户端发现和服务端发现。 我们先来来讨论一下客户端发现。 客户端发现模式当使用客户端发现模式时，客户端负责决定相应服务实例的网络位置，并且对请求实现负载均衡。客户端从一个服务注册服务中查询，其中是所有可用服务实例的库。客户端使用负载均衡算法从多个服务实例中选择出一个，然后发出请求。 下图显示的是这种模式的架构图： 服务实例的网络位置是在启动时注册到服务注册表中，并且在服务终止时从注册表中删除。服务实例注册信息一般是使用心跳机制来定期刷新的。 Netflix OSS提供了一种非常棒的客户端发现模式。Netflix Eureka是一个服务注册表，为服务实例注册管理和查询可用实例提供了REST API接口。Netflix Ribbon是一种IPC客户端，与Eureka合同工作实现对请求的负载均衡。我们会在后面详细讨论Eureka。 客户端发现模式也是优缺点分明。这种模式相对比较直接，而且除了服务注册表，没有其它改变的因素。除此之外，因为客户端知道可用服务注册表信息，因此客户端可以通过使用哈希一致性（hashing consistently）变得更加聪明，更加有效的负载均衡。 而这种模式一个最大的缺点是需要针对不同的编程语言注册不同的服务，在客户端需要为每种语言开发不同的服务发现逻辑。 我们分析过客户端发现后，再看看服务端发现。 服务端发现模式另外一种服务发现的模式是服务端发现模式（server-side discovery pattern），下图展现了这种模式的架构图： 客户端通过负载均衡器向某个服务提出请求，负载均衡器向服务注册表发出请求，将每个请求转发往可用的服务实例。跟客户端发现一样，服务实例在服务注册表中注册或者注销。 AWS Elastic Load Balancer（ELB）是一种服务端发现路由的例子，ELB一般用于均衡从网络来的访问流量，也可以使用ELB来均衡VPC内部的流量。客户端使用DNS，通过ELB发出请求（HTTP或者TCP）。ELB负载均衡器负责在注册的EC2实例或者ECS容器之间均衡负载，并不存在一个分离的服务注册表，而EC2实例和ECS实例也向ELB注册。 HTTP服务和类似NGINX和NGINX Plus的负载均衡器都可以作为服务端发现均衡器。例如，这篇博文就描述如何使用Consul Template来动态配置NGINX反向代理。Consul Template是周期性从存放在Consul Template注册表中配置数据重建配置文件的工具。当文件发生变化时，会运行一个命令。在如上博客中，Consul Template产生了一个nginx.conf文件，用于配置反向代理，然后运行一个命令，告诉NGINX重新调入配置文件。更复杂的例子可以用HTTP API或者DNS动态重新配置NGINX Plus。 某些部署环境，例如Kubernetes和Marathon在集群每个节点上运行一个代理，此代理作为服务端发现负载均衡器。为了向服务发出请求，客户端使用主机IP地址和分配的端口通过代理将请求路由出去。代理将次请求透明的转发到集群中可用的服务实例。 服务端发现模式也有优缺点。最大的优点是客户端无需关注发现的细节，客户端只需要简单的向负载均衡器发送请求，实际上减少了编程语言框架需要完成的发现逻辑。而且，如上说所，某些部署环境免费提供以上功能。 这种模式也有缺陷，除非部署环境提供负载均衡器，否则负载均衡器是另外一个需要配置管理的高可用系统功能 服务注册表服务注册表是服务发现很重要的部分，它是包含服务实例网络地址的数据库。服务注册表需要高可用而且随时更新。客户端可以缓存从服务注册表获得的网络地址。然而，这些信息最终会变得过时，客户端也无法发现服务实例。因此，服务注册表由若干使用复制协议保持同步的服务器构成。 如前所述，Netflix Eureka是一个服务注册表很好地例子，提供了REST API注册和请求服务实例。 服务实例使用POST请求注册网络地址，每30秒必须使用PUT方法更新注册表，使用HTTP DELETE请求或者实例超时来注销。可以想见，客户端可以使用HTTP GET请求接受注册服务实例信息。 Netflix通过在每个AWS EC2域运行一个或者多个Eureka服务实现高可用性，每个Eureka服务器都运行在拥有弹性IP地址的EC2实例上。DNS TEXT记录用于存储Eureka集群配置，其中存放从可用域到一系列Eureka服务器网络地址的列表。当Eureka服务启动时，向DNS请求接受Eureka集群配置，确认同伴位置，给自己分配一个未被使用的弹性IP地址。 Eureka客户端—服务和服务客户端—向DNS请求发现Eureka服务的网络地址，客户端首选使用同一域内的服务。然而，如果没有可用服务，客户端会使用另外一个可用域的Eureka服务。 另外一些服务注册表例子包括： etcd – 是一个高可用，分布式的，一致性的，键值表，用于共享配置和服务发现。两个著名案例包括Kubernetes和Cloud Foundry。 consul – 是一个用于发现和配置的服务。提供了一个API允许客户端注册和发现服务。Consul可以用于健康检查来判断服务可用性。 Apache ZooKeeper – 是一个广泛使用，为分布式应用提供高性能整合的服务。Apache ZooKeeper最初是Hadoop的子项目，现在已经变成顶级项目。 另外，前面强调过，某些系统，例如Kubernetes、Marathon和AWS并没有独立的服务注册表，对他们来说，服务注册表只是一个内置的功能。 现在我们来看看服务注册表的概念，看看服务实例是如何在注册表中注册的。 服务注册选项如前所述，服务实例必须向注册表中注册和注销，如何注册和注销也有一些不同的方式。一种方式是服务实例自己注册，也叫自注册模式（self-registration pattern）；另外一种方式是为其它系统提供服务实例管理的，也叫第三方注册模式（third party registration pattern）。我们来看看自注册模式。 自注册方式当使用自注册模式时，服务实例负责在服务注册表中注册和注销。另外，如果需要的话，一个服务实例也要发送心跳来保证注册信息不会过时。下图描述了这种架构： 一个很好地例子是 Netflix OSS Eureka client。Eureka客户端负责处理服务实例的注册和注销。Spring Cloud project，实现了多种模式，包括服务发现，使得向Eureka服务实例自动注册时更容易。可以用@EnableEurekaClient注释Java配置类。 自注册模式也有优缺点。一个优点是，相对简单，不需要其他系统功能。而一个主要缺点则是，把服务实例跟服务注册表联系起来。必须在每种编程语言和框架内部实现注册代码。 另外一个方法，不需要连接服务和注册表，则是第三方注册模式。 第三方注册模式当使用第三方注册模式时，服务实例并不负责向服务注册表注册，而是由另外一个系统模块，叫做服务管理器，负责注册。服务管理器通过查询部署环境或订阅事件来跟踪运行服务的改变。当管理器发现一个新可用服务，会向注册表注册此服务。服务管理器也负责注销终止的服务实例。下图是这种模式的架构图。 一个服务管理器的例子是开源项目Registrator，负责自动注册和注销被部署为Docker容器的服务实例。Reistrator支持多种服务管理器，包括etcd和Consul。 另外一个服务管理器例子是NetflixOSS Prana，主要面向非JVM语言开发的服务，也称为附带应用（sidecar application），Prana使用Netflix Eureka注册和注销服务实例。 服务管理器是部署环境内置的模块。有自动扩充组创建的EC2实例可以自向ELB自动注册，Kubernetes服务自动注册并且对发现服务可用。 第三方注册模式也是优缺点都有。主要的优点是服务跟服务注册表是分离的，不需要为每种编程语言和架构完成服务注册逻辑，替代的，服务实例是通过一个集中化管理的服务进行管理的。 一个缺点是，除非这种服务被内置于部署环境中，否则也需要配置管理一个高可用的系统。 总结在一个微服务应用中，服务实例运行环境是动态变化的。实例网络地址也是动态变化的，因此，客户端为了访问服务必须使用服务发现机制。 服务发现关键部分是服务注册表，也就是可用服务实例的数据库。服务注册表提供一种注册管理API和请求API。服务实例使用注册管理API来实现注册和注销。 请求API用于发现可用服务实例，相对应的，有两种主要服务发现模式：客户端发现和服务端发现。 在使用客户端发现的系统中，客户端向服务注册表发起请求，选择可用实例，然后发出服务请求 而在使用服务端发现的系统中，客户端通过路由转发请求，路由器向服务注册表发出请求，转发此请求到某个可用实例。 服务实例注册和注销主要有两类方式。一种是服务实例自动注册到服务注册表中，也就是自注册模式；另外一种则是某个系统模块负责处理注册和注销，也就是第三方注册模式。 在某些部署环境中，需要配置自己的服务发现架构，例如：Netflix Eureka、etcd或者Apache ZooKeeper。而在另外一些部署环境中，则自带了这种功能，例如Kubernetes和Marathon 负责处理服务实例的注册和注销。他们也在每个集群节点上运行代理，来实现服务端发现路由器的功能。 HTTP反向代理和负载据衡器（例如NGINX）可以用于服务发现负载均衡器。服务注册表可以将路由信息推送到NGINX，激活一个实时配置更新；例如，可以使用 Consul Template。NGINX Plus 支持额外的动态重新配置机制，可以使用DNS，将服务实例信息从注册表中拉下来，并且提供远程配置的API。 在未来的博客中，我们还将深入探讨微服务其它特点。可以注册NGINX邮件列表来获得最新产品更新提示。 原文链接：Service Discovery in a Microservices Architecture （翻译：杨峰 校对：宋喻） 本系列七篇文章列表如下： 微服务实战（一）：微服务架构的优势与不足 微服务实战（二）：使用API Gateway 微服务实战（三）：深入微服务架构的进程间通信 微服务实战（四）：服务发现的可行方案以及实践案例(本文) 微服务实践（五）：微服务的事件驱动数据管理 微服务实践（六）：选择微服务部署策略 微服务实践（七）：从单体式架构迁移到微服务架构]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>SpringCloud</tag>
        <tag>进程通信</tag>
        <tag>微服务架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务实战（三）：深入微服务架构的进程间通信]]></title>
    <url>%2F2017%2F11%2F05%2FBuilding-Microservices-Inter-Process-Communication-in-a-Microservices-Architecture%2F</url>
    <content type="text"><![CDATA[摘要这是采用微服务架构创建自己应用系列第三篇文章。第一篇介绍了微服务架构模式，和单体式模式进行了比较，并且讨论了使用微服务架构的优缺点。第二篇描述了采用微服务架构应用客户端之间如何采用API Gateway方式进行通信。在这篇文章中，我们将讨论系统服务之间如何通信。 简介在单体式应用中，各个模块之间的调用是通过编程语言级别的方法或者函数来实现的。但是一个基于微服务的分布式应用是运行在多台机器上的。一般来说，每个服务实例都是一个进程。因此，如下图所示，服务之间的交互必须通过进程间通信（IPC）来实现。 后面我们将会详细介绍IPC技术，现在我们先来看下设计相关的问题。 交互模式当为某一个服务选择IPC时，首先需要考虑服务之间如何交互。客户端和服务器之间有很多的交互模式，我们可以从两个维度进行归类。第一个维度是一对一还是一对多： 一对一：每个客户端请求有一个服务实例来响应。 一对多：每个客户端请求有多个服务实例来响应 第二个维度是这些交互式同步还是异步： 同步模式：客户端请求需要服务端即时响应，甚至可能由于等待而阻塞。 异步模式：客户端请求不会阻塞进程，服务端的响应可以是非即时的。 下表显示了不同交互模式： 一对一的交互模式有以下几种方式： 请求/响应：一个客户端向服务器端发起请求，等待响应。客户端期望此响应即时到达。在一个基于线程的应用中，等待过程可能造成线程阻塞。 通知（也就是常说的单向请求）：一个客户端请求发送到服务端，但是并不期望服务端响应。 请求/异步响应：客户端发送请求到服务端，服务端异步响应请求。客户端不会阻塞，而且被设计成默认响应不会立刻到达。 一对多的交互模式有以下几种方式： 发布/ 订阅模式：客户端发布通知消息，被零个或者多个感兴趣的服务消费。 发布/异步响应模式：客户端发布请求消息，然后等待从感兴趣服务发回的响应。 每个服务都是以上这些模式的组合，对某些服务，一个IPC机制就足够了；而对另外一些服务则需要多种IPC机制组合。下图展示了在一个打车服务请求中服务之间是如何通信的。 上图中的服务通信使用了通知、请求/响应、发布/订阅等方式。例如，乘客通过移动端给『行程管理服务』发送通知，希望申请一次出租服务。『行程管理服务』发送请求/响应消息给『乘客服务』以确认乘客账号是有效的。紧接着创建此次行程，并用发布/订阅交互模式通知其他服务，包括定位可用司机的调度服务。 现在我们了解了交互模式，接下来我们一起来看看如何定义API。 定义APIAPI是服务端和客户端之间的契约。不管选择了什么样的IPC机制，重要的是使用某种交互式定义语言（IDL）来精确定义一个服务的API。甚至有一些关于使用API first的方法（API-first approach）来定义服务的很好的理由。在开发之前，你需要先定义服务的接口，并与客户端开发者详细讨论确认。这样的讨论和设计会大幅度提到API的可用度以及满意度。 在本文后半部分你将会看到，API定义实质上依赖于选择哪种IPC。如果使用消息机制，API则由消息频道（channel）和消息类型构成；如果选择使用HTTP机制，API则由URL和请求、响应格式构成。后面将会详细描述IDL。 API的演化服务端API会不断变化。在一个单体式应用中经常会直接修改API，然后更新给所有的调用者。而在基于微服务架构应用中，这很困难，即使只有一个服务使用这个API，不可能强迫用户跟服务端保持同步更新。另外，开发者可能会尝试性的部署新版本的服务，这个时候，新旧服务就会同事运行。你需要知道如何处理这些问题。 你如何处理API变化，这依赖于这些变化有多大。某些改变是微小的，并且可以和之前版本兼容。比如，你可能只是为某个请求和响应添加了一个属性。设计客户端和服务端时候应该遵循健壮性原理，这很重要。客户端使用旧版API应该也能和新版本一起工作。服务端仍然提供默认响应值，客户端忽略此版本不需要的响应。使用IPC机制和消息格式对于API演化很有帮助。 但是有时候，API需要进行大规模的改动，并且可能与之前版本不兼容。因为你不可能强制让所有的客户端立即升级，所以支持老版本客户端的服务还需要再运行一段时间。如果你正在使用基于基于HTTP机制的IPC，例如REST，一种解决方案是把版本号嵌入到URL中。每个服务都可能同时处理多个版本的API。或者，你可以部署多个实例，每个实例负责处理一个版本的请求。 处理部分失败在上一篇关于API gateway的文章中，我们了解到分布式系统中部分失败是普遍存在的问题。因为客户端和服务端是都是独立的进程，一个服务端有可能因为故障或者维护而停止服务，或者此服务因为过载停止或者反应很慢。 考虑这篇文章中描述的部分失败的场景。假设推荐服务无法响应请求，那客户端就会由于等待响应而阻塞，这不仅会给客户带来很差的体验，而且在很多应用中还会占用很多资源，比如线程，以至于到最后由于等待响应被阻塞的客户端越来越多，线程资源被耗费完了。如下图所示： 为了预防这种问题，设计服务时候必须要考虑部分失败的问题。 Netfilix提供了一个比较好的解决方案，具体的应对措施包括： • 网络超时：当等待响应时，不要无限期的阻塞，而是采用超时策略。使用超时策略可以确保资源不会无限期的占用。• 限制请求的次数：可以为客户端对某特定服务的请求设置一个访问上限。如果请求已达上限，就要立刻终止请求服务。• 断路器模式（Circuit Breaker Pattern）：记录成功和失败请求的数量。如果失效率超过一个阈值，触发断路器使得后续的请求立刻失败。如果大量的请求失败，就可能是这个服务不可用，再发请求也无意义。在一个失效期后，客户端可以再试，如果成功，关闭此断路器。• 提供回滚：当一个请求失败后可以进行回滚逻辑。例如，返回缓存数据或者一个系统默认值。 Netflix Hystrix是一个实现相关模式的开源库。如果使用JVM，推荐考虑使用Hystrix。而如果使用非JVM环境，你可以使用类似功能的库。 IPC技术现在有很多不同的IPC技术。服务之间的通信可以使用同步的请求/响应模式，比如基于HTTP的REST或者Thrift。另外，也可以选择异步的、基于消息的通信模式，比如AMQP或者STOMP。除以之外，还有其它的消息格式供选择，比如JSON和XML，它们都是可读的，基于文本的消息格式。当然，也还有二进制格式（效率更高）的，比如Avro和Protocol Buffer。接下来我们将会讨论异步的IPC模式和同步的IPC模式，首先来看异步的。异步的，基于消息通信 当使用基于异步交换消息的进程通信方式时，一个客户端通过向服务端发送消息提交请求。如果服务端需要回复，则会发送另外一个独立的消息给客户端。因为通信是异步的，客户端不会因为等待而阻塞，相反，客户端理所当然的认为响应不会立刻接收到。 一个消息由头部（元数据例如发送方）和消息体构成。消息通过channel发送，任何数量的生产者都可以发送消息到channel，同样的，任何数量的消费者都可以从渠道中接受数据。有两类channel，点对点和发布/订阅。点对点channel会把消息准确的发送到某个从channel读取消息的消费者，服务端使用点对点来实现之前提到的一对一交互模式；而发布/订阅则把消息投送到所有从channel读取数据的消费者，服务端使用发布/订阅channel来实现上面提到的一对多交互模式。 下图展示了打车软件如何使用发布/订阅： 行程管理服务在发布-订阅channel内创建一个行程消息，并通知调度服务有一个新的行程请求，调度服务发现一个可用的司机然后向发布-订阅channel写入司机建议消息（Driver Proposed message）来通知其他服务。 有很多消息系统可以选择，最好选择一种支持多编程语言的。一些消息系统支持标准协议，例如AMQP和STOMP。其他消息系统则使用独有的协议，有大量开源消息系统可选，比如RabbitMQ、Apache Kafka、Apache ActiveMQ和NSQ。它们都支持某种形式的消息和channel，并且都是可靠的、高性能和可扩展的；然而，它们的消息模型完全不同。 使用消息机制有很多优点： 解耦客户端和服务端：客户端只需要将消息发送到正确的channel。客户端完全不需要了解具体的服务实例，更不需要一个发现机制来确定服务实例的位置。 Message Buffering：在一个同步请求/响应协议中，例如HTTP，所有的客户端和服务端必须在交互期间保持可用。而在消息模式中，消息broker将所有写入channel的消息按照队列方式管理，直到被消费者处理。也就是说，在线商店可以接受客户订单，即使下单系统很慢或者不可用，只要保持下单消息进入队列就好了。 弹性客户端-服务端交互：消息机制支持以上说的所有交互模式。 直接进程间通信：基于RPC机制，试图唤醒远程服务看起来跟唤醒本地服务一样。然而，因为物理定律和部分失败可能性，他们实际上非常不同。消息使得这些不同非常明确，开发者不会出现问题。 然而，消息机制也有自己的缺点： 额外的操作复杂性：消息系统需要单独安装、配置和部署。消息broker（代理）必须高可用，否则系统可靠性将会受到影响。 实现基于请求/响应交互模式的复杂性：请求/响应交互模式需要完成额外的工作。每个请求消息必须包含一个回复渠道ID和相关ID。服务端发送一个包含相关ID的响应消息到channel中，使用相关ID来将响应对应到发出请求的客户端。也许这个时候，使用一个直接支持请求/响应的IPC机制会更容易些。 现在我们已经了解了基于消息的IPC，接下来我们来看看基于请求/响应模式的IPC。 同步的，基于请求/响应的IPC 当使用一个同步的，基于请求/响应的IPC机制，客户端向服务端发送一个请求，服务端处理请求，返回响应。一些客户端会由于等待服务端响应而被阻塞，而另外一些客户端也可能使用异步的、基于事件驱动的客户端代码（Future或者Rx Observable的封装）。然而，不像使用消息机制，客户端需要响应及时返回。这个模式中有很多可选的协议，但最常见的两个协议是REST和Thrift。首先我们来看下REST。 REST现在很流行使用RESTful风格的API。REST是基于HTTP协议的。另外，一个需要理解的比较重要的概念是，REST是一个资源，一般代表一个业务对象，比如一个客户或者一个产品，或者一组商业对象。REST使用HTTP语法协议来修改资源，一般通过URL来实现。举个例子，GET请求返回一个资源的简单信息，响应格式通常是XML或者JSON对象格式。POST请求会创建一个新资源，PUT请求更新一个资源。这里引用下REST之父Roy Fielding说的： 当需要一个整体的、重视模块交互可扩展性、接口概括性、组件部署独立性和减小延迟、提供安全性和封装性的系统时，REST可以提供这样一组满足需求的架构。 下图展示了打车软件是如何使用REST的。 乘客通过移动端向行程管理服务的/trips资源提交了一个POST请求。行程管理服务收到请求之后，会发送一个GET请求到乘客管理服务以获取乘客信息。当确认乘客信息之后，紧接着会创建一个行程，并向移动端返回201（译者注：状态码）响应。 很多开发者都表示他们基于HTTP的API是RESTful的。但是，如同Fielding在他的博客中所说，这些API可能并不都是RESTful的。Leonard Richardson为REST定义了一个成熟度模型，具体包含以下4个层次（摘自IBM）： 第一个层次（Level 0）的 Web 服务只是使用 HTTP 作为传输方式，实际上只是远程方法调用（RPC）的一种具体形式。SOAP 和 XML-RPC 都属于此类。 第二个层次（Level 1）的 Web 服务引入了资源的概念。每个资源有对应的标识符和表达。 第三个层次（Level 2）的 Web 服务使用不同的 HTTP 方法来进行不同的操作，并且使用 HTTP 状态码来表示不同的结果。如 HTTP GET 方法来获取资源，HTTP DELETE 方法来删除资源。 第四个层次（Level 3）的 Web 服务使用 HATEOAS。在资源的表达中包含了链接信息。客户端可以根据链接来发现可以执行的动作。 使用基于HTTP的协议有如下好处： • HTTP非常简单并且大家都很熟悉。• 可以使用浏览器扩展（比如Postman）或者curl之类的命令行来测试API。• 内置支持请求/响应模式的通信。• HTTP对防火墙友好的。• 不需要中间代理，简化了系统架构。 不足之处包括： 只支持请求/响应模式交互。可以使用HTTP通知，但是服务端必须一直发送HTTP响应才行。 因为客户端和服务端直接通信（没有代理或者buffer机制），在交互期间必须都在线。 客户端必须知道每个服务实例的URL。如之前那篇关于API Gateway的文章所述，这也是个烦人的问题。客户端必须使用服务实例发现机制。 开发者社区最近重新发现了RESTful API接口定义语言的价值。于是就有了一些RESTful风格的服务框架，包括RAML和Swagger。一些IDL，例如Swagger允许定义请求和响应消息的格式。其它的，例如RAML，需要使用另外的标识，例如JSON Schema。对于描述API，IDL一般都有工具来定义客户端和服务端骨架接口。 ThriftApache Thrift是一个很有趣的REST的替代品。它是Facebook实现的一种高效的、支持多种编程语言的远程服务调用的框架。Thrift提供了一个C风格的IDL定义API。使用Thrift编译器可以生成客户端和服务器端代码框架。编译器可以生成多种语言的代码，包括C++、Java、Python、PHP、Ruby, Erlang和Node.js。 Thrift接口包括一个或者多个服务。服务定义类似于一个JAVA接口，是一组方法。Thrift方法可以返回响应，也可以被定义为单向的。返回值的方法其实就是请求/响应类型交互模式的实现。客户端等待响应，并可能抛出异常。单向方法对应于通知类型的交互模式，服务端并不返回响应。 Thrift支持多种消息格式：JSON、二进制和压缩二进制。二进制比JSON更高效，因为二进制解码更快。同样原因，压缩二进制格式可以提供更高级别的压缩效率。JSON，是易读的。Thrift也可以在裸TCP和HTTP中间选择，裸TCP看起来比HTTP更加有效。然而，HTTP对防火墙，浏览器和人来说更加友好。消息格式 了解完HTTP和Thrift后，我们来看下消息格式方面的问题。如果使用消息系统或者REST，就可以选择消息格式。其它的IPC机制，例如Thrift可能只支持部分消息格式，也许只有一种。无论哪种方式，我们必须使用一个跨语言的消息格式，这非常重要。因为指不定哪天你会使用其它语言。 有两类消息格式：文本和二进制。文本格式的例子包括JSON和XML。这种格式的优点在于不仅可读，而且是自描述的。在JSON中，一个对象就是一组键值对。类似的，在XML中，属性是由名字和值构成。消费者可以从中选择感兴趣的元素而忽略其它部分。同时，小幅度的格式修改可以很容器向后兼容。 XML文档结构是由XML schema定义的。随着时间发展，开发者社区意识到JSON也需要一个类似的机制。一个选择是使用JSON Schema，要么是独立的，要么是例如Swagger的IDL。 基于文本的消息格式最大的缺点是消息会变得冗长，特别是XML。因为消息是自描述的，所以每个消息都包含属性和值。另外一个缺点是解析文本的负担过大。所以，你可能需要考虑使用二进制格式。 二进制的格式也有很多。如果使用的是Thrift RPC，那可以使用二进制Thrift。如果选择消息格式，常用的还包括Protocol Buffers和Apache Avro。它们都提供典型的IDL来定义消息架构。一个不同点在于Protocol Buffers使用的是加标记（tag）的字段，而Avro消费者需要知道模式（schema）来解析消息。因此，使用前者，API更容易演进。这篇博客很好的比较了Thrift、Protocol Buffers、Avro三者的区别。 总结微服务必须使用进程间通信机制来交互。当设计服务的通信模式时，你需要考虑几个问题：服务如何交互，每个服务如何标识API，如何升级API，以及如何处理部分失败。微服务架构有两类IPC机制可选，异步消息机制和同步请求/响应机制。在下一篇文章中，我们将会讨论微服务架构中的服务发现问题。 原文链接：Building Microservices: Inter-Process Communication in a Microservices Architecture（翻译：杨峰 校对：李颖杰） 本系列七篇文章列表如下： 微服务实战（一）：微服务架构的优势与不足 微服务实战（二）：使用API Gateway 微服务实战（三）：深入微服务架构的进程间通信(本文) 微服务实战（四）：服务发现的可行方案以及实践案例 微服务实践（五）：微服务的事件驱动数据管理 微服务实践（六）：选择微服务部署策略 微服务实践（七）：从单体式架构迁移到微服务架构]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>SpringCloud</tag>
        <tag>进程通信</tag>
        <tag>微服务架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务实战（二）：使用API Gateway]]></title>
    <url>%2F2017%2F11%2F05%2FBuilding-Microservices-Using-an-API-Gateway%2F</url>
    <content type="text"><![CDATA[摘要本系列的第一篇介绍了微服务架构模式。它讨论了采用微服务的优点和缺点，除了一些复杂的微服务，这种模式还是复杂应用的理想选择。 当你决定将应用作为一组微服务时，需要决定应用客户端如何与微服务交互。在单体式程序中，通常只有一组冗余的或者负载均衡的服务提供点。在微服务架构中，每一个微服务暴露一组细粒度的服务提供点。在本篇文章中，我们来看它如何影响客户端到服务端通信，同时提出一种API Gateway的方法。 原文地址: Building Microservices: Using an API Gateway 介绍假定你正在为在线购物应用开发一个原生手机客户端。你需要实现一个产品最终页来展示商品信息。 例如，下面的图展示了你在亚马逊Android客户端上滑动产品最终页时看到的信息。 虽然这是一个智能手机应用，这个产品最终页展示了非常多的信息。例如，不仅这里有产品基本信息（名字、描述和价格），还有以下内容： 购物车中的物品数 下单历史 用户评论 低库存警告 快递选项 各式各样的推荐，包括经常跟这个物品一起被购买的产品、购买该物品的其他顾客购买的产品以及购买该产品的顾客还浏览了哪些产品。 可选的购物选项 当采用一个单体式应用架构，一个移动客户端将会通过一个REST请求（GET api.company.com/productdetails/productId）来获取这些数据。一个负载均衡将请求分发到多个应用实例之一。应用将查询各种数据库并返回请求给客户端。 相对的，若是采用微服务架构，最终页上的数据会分布在不同的微服务上。下面列举了可能与产品最终页数据有关的一些微服务： 购物车服务 – 购物车中的物品数 下单服务 – 下单历史 分类服务 – 基本产品信息，如名字、图片和价格 评论服务 – 用户评论 库存服务 – 低库存警告 快递服务 – 快递选项、截止时间、来自不同快递API的成本计算 推荐服务 – 推荐产品 我们需要决定移动客户端如何访问这些服务。请看下面这几种方式 客户端到微服务直接通信理论上说，一个客户端可以直接给多个微服务中的任何一个发起请求。每一个微服务都会有一个对外服务端https://serviceName.api.company.name。这个URL可能会映射到微服务的负载均衡上，它再转发请求到具体节点上。为了搜索产品细节，移动端需要向上述微服务逐个发请求。 不幸的是，这个方案有很多困难和限制。其中一个问题是客户端的需求量与每个微服务暴露的细粒度API数量的不匹配。如图中，客户端需要7次单独请求。在更复杂的场景中，可能会需要更多次请求。例如，亚马逊的产品最终页要请求数百个微服务。虽然一个客户端可以通过LAN发起很多个请求，但是在公网上这样会很没有效率，这个问题在移动互联网上尤为突出。这个方案同时会导致客户端代码非常复杂。 另一个存在的问题是客户端直接请求微服务的协议可能并不是web友好型。一个服务可能是用Thrift的RPC协议，而另一个服务可能是用AMQP消息协议。它们都不是浏览或防火墙友好的，并且最好是内部使用。应用应该在防火墙外采用类似HTTP或者WEBSocket协议。 这个方案的另一个缺点是它很难重构微服务。随着时间的推移，我们可能需要改变系统微服务目前的切分方案。例如，我们可能需要将两个服务合并或者将一个服务拆分为多个。但是，如果客户端直接与微服务交互，那么这种重构就很难实施。 由于上述三种问题的原因，客户端直接与服务器端通信的方式很少在实际中使用。 采用一个API Gateway通常来说，一个更好的解决办法是采用API Gateway的方式。API Gateway是一个服务器，也可以说是进入系统的唯一节点。这跟面向对象设计模式中的Facade模式很像。API Gateway封装内部系统的架构，并且提供API给各个客户端。它还可能有其他功能，如授权、监控、负载均衡、缓存、请求分片和管理、静态响应处理等。下图展示了一个适应当前架构的API Gateway。 API Gateway负责请求转发、合成和协议转换。所有来自客户端的请求都要先经过API Gateway，然后路由这些请求到对应的微服务。API Gateway将经常通过调用多个微服务来处理一个请求以及聚合多个服务的结果。它可以在web协议与内部使用的非Web友好型协议间进行转换，如HTTP协议、WebSocket协议。 API Gateway可以提供给客户端一个定制化的API。它暴露一个粗粒度API给移动客户端。以产品最终页这个使用场景为例。API Gateway提供一个服务提供点（/productdetails?productid=xxx）使得移动客户端可以在一个请求中检索到产品最终页的全部数据。API Gateway通过调用多个服务来处理这一个请求并返回结果，涉及产品信息、推荐、评论等。 一个很好的API Gateway例子是Netfix API Gateway。Netflix流服务提供数百个不同的微服务，包括电视、机顶盒、智能手机、游戏系统、平板电脑等。起初，Netflix视图提供一个适用全场景的API。但是，他们发现这种形式不好用，因为涉及到各式各样的设备以及它们独特的需求。现在，他们采用一个API Gateway来提供容错性高的API，针对不同类型设备有相应代码。事实上，一个适配器处理一个请求平均要调用6到8个后端服务。Netflix API Gateway每天处理数十亿的请求。 API Gateway的优点和缺点如你所料，采用API Gateway也是优缺点并存的。API Gateway的一个最大好处是封装应用内部结构。相比起来调用指定的服务，客户端直接跟gatway交互更简单点。API Gateway提供给每一个客户端一个特定API，这样减少了客户端与服务器端的通信次数，也简化了客户端代码。 API Gateway也有一些缺点。它是一个高可用的组件，必须要开发、部署和管理。还有一个问题，它可能成为开发的一个瓶颈。开发者必须更新API Gateway来提供新服务提供点来支持新暴露的微服务。更新API Gateway时必须越轻量级越好。否则，开发者将因为更新Gateway而排队列。但是，除了这些缺点，对于大部分的应用，采用API Gateway的方式都是有效的。 实现一个API Gateway既然我们已经知道了采用API Gateway的动机和优缺点，下面来看在设计它时需要考虑哪些事情。 性能和可扩展性只有少数公司需要处理像Netflix那样的规模，每天需要处理数十亿的请求。但是，对于大多数应用，API Gateway的性能和可扩展性也是非常重要的。因此，创建一个支持同步、非阻塞I/O的API Gateway是有意义的。已经有不同的技术可以用来实现一个可扩展的API Gateway。在JVM上，采用基于NIO技术的框架，如Netty，Vertx，Spring Reactor或者JBoss Undertow。Node.js是一个非JVM的流行平台，它是一个在Chrome的JavaScript引擎基础上建立的平台。一个可选的方案是NGINX Plus。NGINX Plus提供一个成熟的、可扩展的、高性能web服务器和反向代理，它们均容易部署、配置和二次开发。NGINX Plus可以管理授权、权限控制、负载均衡、缓存并提供应用健康检查和监控。 采用反应性编程模型对于有些请求，API Gateway可以通过直接路由请求到对应的后端服务上的方式来处理。对于另外一些请求，它需要调用多个后端服务并合并结果来处理。对于一些请求，例如产品最终页面请求，发给后端服务的请求是相互独立的。为了最小化响应时间，API Gateway应该并发的处理相互独立的请求。但是，有时候请求之间是有依赖的。API Gateway可能需要先通过授权服务来验证请求，然后在路由到后端服务。类似的，为了获得客户的产品愿望清单，需要先获取该用户的资料，然后返回清单上产品的信息。这样的一个API 组件是Netflix Video Grid。 利用传统的同步回调方法来实现API合并的代码会使得你进入回调函数的噩梦中。这种代码将非常难度且难以维护。一个优雅的解决方案是采用反应性编程模式来实现。类似的反应抽象实现有Scala的Future，Java8的CompletableFuture和JavaScript的Promise。基于微软.Net平台的有Reactive Extensions(Rx)。Netflix为JVM环境创建了RxJava来使用他们的API Gateway。同样地，JavaScript平台有RxJS，可以在浏览器和Node.js平台上运行。采用反应编程方法可以帮助快速实现一个高效的API Gateway代码。 服务调用一个基于微服务的应用是一个分布式系统，并且必须采用线程间通信的机制。有两种线程间通信的方法。一种是采用异步机制，基于消息的方法。这类的实现方法有JMS和AMQP。另外的，例如Zeromq属于服务间直接通信。还有一种线程间通信采用同步机制，例如Thrift和HTTP。事实上一个系统会同时采用同步和异步两种机制。由于它的实现方式有很多种，因此API Gateway就需要支持多种通信方式。 服务发现API Gateway需要知道每一个微服务的IP和端口。在传统应用中，你可能会硬编码这些地址，但是在现在云基础的微服务应用中，这将是个简单的问题。基础服务通常会采用静态地址，可以采用操作系统环境变量来指定。但是，探测应用服务的地址就没那么容易了。应用服务通常动态分配地址和端口。同样的，由于扩展或者升级，服务的实例也会动态的改变。因此，API Gateway需要采用系统的服务发现机制，要么采用服务端发现，要么是客户端发现。后续的一篇文章将会更详细的介绍这部分。如果采用客户端发现服务，API Gateway必须要去查询服务注册处，也就是微服务实例地址的数据库。 处理部分失败在实现API Gateway过程中，另外一个需要考虑的问题就是部分失败。这个问题发生在分布式系统中当一个服务调用另外一个服务超时或者不可用的情况。API Gateway不应该被阻断并处于无限期等待下游服务的状态。但是，如何处理这种失败依赖于特定的场景和具体服务。例如，如果是在产品详情页的推荐服务模块无响应，那么API Gateway应该返回剩下的其他信息给用户，因为这些信息也是有用的。推荐部分可以返回空，也可以返回固定的顶部10个给用户。但是，如果是产品信息服务无响应，那么API Gateway就应该给客户端返回一个错误。 在缓存有效的时候，API Gateway应该能够返回缓存。例如，由于产品价格变化并不频繁，API Gateway在价格服务不可用时应该返回缓存中的数值。这类数据可以由API Gateway自身来缓存，也可以由Redis或Memcached这类外部缓存实现。通过返回缓存数据或者默认数据，API Gateway来确保系统错误不影响到用户体验。 Netflix Hystrix对于实现远程服务调用代码来说是一个非常好用的库。Hystrix记录那些超过预设定的极限值的调用。它实现了circuit break模式，使得可以将客户端从无响应服务的无尽等待中停止。如果一个服务的错误率超过预设值，Hystrix将中断服务，并且在一段时间内所有请求立刻失效。Hystrix可以为请求失败定义一个fallback操作，例如读取缓存或者返回默认值。如果你在用JVM，就应该考虑使用Hystrix。如果你采用的非JVM环境，那么应该考虑采用类似功能的库。 总结对于大多数微服务基础的应用，实现一个API Gateway都是有意义的，它就像是进入系统的一个服务提供点。API Gateway负责请求转发、请求合成和协议转换。它提供给应用客户端一个自定义的API。API Gateway可以通过返回缓存或者默认值的方式来掩盖后端服务的错误。在本系列的下一篇文章中，我们将讨论服务间的通信问题。 本系列七篇文章列表如下： 微服务实战（一）：微服务架构的优势与不足 微服务实战（二）：使用API Gateway（本文） 微服务实战（三）：深入微服务架构的进程间通信 微服务实战（四）：服务发现的可行方案以及实践案例 微服务实践（五）：微服务的事件驱动数据管理 微服务实践（六）：选择微服务部署策略 微服务实践（七）：从单体式架构迁移到微服务架构]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>SpringCloud</tag>
        <tag>API 网关</tag>
        <tag>API gateway</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务实战（一）：微服务架构的优势与不足]]></title>
    <url>%2F2017%2F11%2F05%2Fintroduction-to-microservices%2F</url>
    <content type="text"><![CDATA[希望读者通过本系列文章对微服务优缺点有一个比较好的理解，以及何时使用这种架构。也许微服务架构比较适合你的应用。也许你正在开发一个大型、复杂单体式应用，日常开发和部署经验非常缓慢和痛苦，而微服务看起来是远方一个极乐世界。幸运的是，有可以参考的脱离苦海的策略，本篇文章中，我将描述如何逐步将单体式应用迁移到微服务架构。 本系列七篇文章列表如下： 微服务实战（一）：微服务架构的优势与不足 微服务实战（二）：使用API Gateway 微服务实战（三）：深入微服务架构的进程间通信 微服务实战（四）：服务发现的可行方案以及实践案例 微服务实践（五）：微服务的事件驱动数据管理 微服务实践（六）：选择微服务部署策略 微服务实践（七）：从单体式架构迁移到微服务架构 摘要本文来自Nginx官方博客Introduction to Microservices，是微服务系列文章的第一篇，主要探讨了传统的单体式应用的不足，以及微服务架构的优势与挑战。正如作者所说，微服务架构更适合用于构建复杂的应用，尽管它也有自己的不足。 这篇文章作者是Chris Richardson，他是早期基于Java的Amazonite EC2 PaaS平台CloudFoundry.com的创始人。现在他为企业提供如何开发和部署应用的咨询服务。他也经常在http://microservices.io上发表有关微服务的文章。 微服务正在博客、社交媒体讨论组和会议演讲中获得越来越多的关注，在Gartner的2014 Hype Cycle上它的排名非常靠前。同时，软件社区中也有不少持怀疑论者，认为微服务不是什么新东西。Naysayers认为这就是SOA架构的重新包装。然而，尽管存在着不同的争论，微服务架构模式却正在为敏捷部署以及复杂企业应用实施提供巨大的帮助。 这篇博客是关于如何设计、开发和部署微服务的七篇系列文章中的第一篇。读者将会从中学到方法，并且和单体式架构模式（译者注：本文中会将 Monolithic翻译为单体）进行对比。这一系列文章将描述微服务架构中不同元素。你将了解到微服务架构模式的优缺点，以便决定是否更好的将微服务架构应用到自己的项目中，以及如何应用这一模式。 首先我们看看为什么要考虑使用微服务。 开发单体式应用假设你正准备开发一款与Uber和Hailo竞争的出租车调度软件，经过初步会议和需求分析，你可能会手动或者使用基于Rails、Spring Boot、Play或者Maven的生成器开始这个新项目，它的六边形架构是模块化的 ，架构图如下： 应用核心是业务逻辑，由定义服务、域对象和事件的模块完成。围绕着核心的是与外界打交道的适配器。适配器包括数据库访问组件、生产和处理消息的消息组件，以及提供API或者UI访问支持的web模块等。 尽管也是模块化逻辑，但是最终它还是会打包并部署为单体式应用。具体的格式依赖于应用语言和框架。例如，许多Java应用会被打包为WAR格式，部署在Tomcat或者Jetty上，而另外一些Java应用会被打包成自包含的JAR格式，同样，Rails和Node.js会被打包成层级目录。 这种应用开发风格很常见，因为IDE和其它工具都擅长开发一个简单应用，这类应用也很易于调试，只需要简单运行此应用，用Selenium链接UI就可以完成端到端测试。单体式应用也易于部署，只需要把打包应用拷贝到服务器端，通过在负载均衡器后端运行多个拷贝就可以轻松实现应用扩展。在早期这类应用运行的很好。 单体式应用的不足不幸的是，这种简单方法却有很大的局限性。一个简单的应用会随着时间推移逐渐变大。在每次的sprint中，开发团队都会面对新“故事”，然后开发许多新代码。几年后，这个小而简单的应用会变成了一个巨大的怪物。这儿有一个例子，我最近和一个开发者讨论，他正在写一个工具，用来分析他们一个拥有数百万行代码的应用中JAR文件之间的依赖关系。我很确信这个代码正是很多开发者经过多年努力开发出来的一个怪物。 一旦你的应用变成一个又大又复杂的怪物，那开发团队肯定很痛苦。敏捷开发和部署举步维艰，其中最主要问题就是这个应用太复杂，以至于任何单个开发者都不可能搞懂它。因此，修正bug和正确的添加新功能变的非常困难，并且很耗时。另外，团队士气也会走下坡路。如果代码难于理解，就不可能被正确的修改。最终会走向巨大的、不可理解的泥潭。 单体式应用也会降低开发速度。应用越大，启动时间会越长。比如，最近的一个调查表明，有时候应用的启动时间居然超过了12分钟。我还听说某些应用需要40分钟启动时间。如果开发者需要经常重启应用，那么大部分时间就要在等待中渡过，生产效率受到极大影响。 另外，复杂而巨大的单体式应用也不利于持续性开发。今天，SaaS应用常态就是每天会改变很多次，而这对于单体式应用模式非常困难。另外，这种变化带来的影响并没有很好的被理解，所以不得不做很多手工测试。那么接下来，持续部署也会很艰难。 单体式应用在不同模块发生资源冲突时，扩展将会非常困难。比如，一个模块完成一个CPU敏感逻辑，应该部署在AWS EC2 Compute Optimized instances，而另外一个内存数据库模块更合适于EC2 Memory-optimized instances。然而，由于这些模块部署在一起，因此不得不在硬件选择上做一个妥协。 单体式应用另外一个问题是可靠性。因为所有模块都运行在一个进程中，任何一个模块中的一个bug，比如内存泄露，将会有可能弄垮整个进程。除此之外，因为所有应用实例都是唯一的，这个bug将会影响到整个应用的可靠性。 最后，单体式应用使得采用新架构和语言非常困难。比如，设想你有两百万行采用XYZ框架写的代码。如果想改成ABC框架，无论是时间还是成本都是非常昂贵的，即使ABC框架更好。因此，这是一个无法逾越的鸿沟。你不得不在最初选择面前低头。 总结一下：一开始你有一个很成功的关键业务应用，后来就变成了一个巨大的，无法理解的怪物。因为采用过时的，效率低的技术，使得雇佣有潜力的开发者很困难。应用无法扩展，可靠性很低，最终，敏捷性开发和部署变的无法完成。 那么如何应对呢？ 微处理架构——处理复杂事物许多公司，比如Amazon、eBay和NetFlix，通过采用微处理结构模式解决了上述问题。其思路不是开发一个巨大的单体式的应用，而是将应用分解为小的、互相连接的微服务。 一个微服务一般完成某个特定的功能，比如下单管理、客户管理等等。每一个微服务都是微型六角形应用，都有自己的业务逻辑和适配器。一些微服务还会发布API给其它微服务和应用客户端使用。其它微服务完成一个Web UI，运行时，每一个实例可能是一个云VM或者是Docker容器。 比如，一个前面描述系统可能的分解如下： 每一个应用功能区都使用微服务完成，另外，Web应用会被拆分成一系列简单的Web应用（比如一个对乘客，一个对出租车驾驶员）。这样的拆分对于不同用户、设备和特殊应用场景部署都更容易。 每一个后台服务开放一个REST API，许多服务本身也采用了其它服务提供的API。比如，驾驶员管理使用了告知驾驶员一个潜在需求的通知服务。UI服务激活其它服务来更新Web页面。所有服务都是采用异步的，基于消息的通讯。微服务内部机制将会在后续系列中讨论。 一些REST API也对乘客和驾驶员采用的移动应用开放。这些应用并不直接访问后台服务，而是通过API Gateway来传递中间消息。API Gateway负责负载均衡、缓存、访问控制、API 计费监控等等任务，可以通过NGINX方便实现，后续文章将会介绍到API Gateway。 微服务架构模式在上图中对应于代表可扩展Scale Cube的Y轴，这是一个在《The Art of Scalability》书中描述过的三维扩展模型。另外两个可扩展轴，X轴由负载均衡器后端运行的多个应用副本组成，Z轴是将需求路由到相关服务。 应用基本可以用以上三个维度来表示，Y轴代表将应用分解为微服务。运行时，X轴代表运行多个隐藏在负载均衡器之后的实例，提供吞吐能力。一些应用可能还是用Z轴将服务分区。下面的图演示行程管理服务如何部署在运行于AWS EC2上的Docker上。 运行时，行程管理服务由多个服务实例构成。每一个服务实例都是一个Docker容器。为了保证高可用，这些容器一般都运行在多个云VM上。服务实例前是一层诸如NGINX的负载均衡器，他们负责在各个实例间分发请求。负载均衡器也同时处理其它请求，例如缓存、权限控制、API统计和监控。 这种微服务架构模式深刻影响了应用和数据库之间的关系，不像传统多个服务共享一个数据库，微服务架构每个服务都有自己的数据库。另外，这种思路也影响到了企业级数据模式。同时，这种模式意味着多份数据，但是，如果你想获得微服务带来的好处，每个服务独有一个数据库是必须的，因为这种架构需要这种松耦合。下面的图演示示例应用数据库架构。 每种服务都有自己的数据库，另外，每种服务可以用更适合自己的数据库类型，也被称作多语言一致性架构。比如，驾驶员管理（发现哪个驾驶员更靠近乘客），必须使用支持地理信息查询的数据库。 表面上看来，微服务架构模式有点像SOA，他们都由多个服务构成。但是，可以从另外一个角度看此问题，微服务架构模式是一个不包含Web服务（WS-）和ESB服务的SOA。微服务应用乐于采用简单轻量级协议，比如REST，而不是WS-，在微服务内部避免使用ESB以及ESB类似功能。微服务架构模式也拒绝使用canonical schema等SOA概念。 微服务架构的好处微服务架构模式有很多好处。首先，通过分解巨大单体式应用为多个服务方法解决了复杂性问题。在功能不变的情况下，应用被分解为多个可管理的分支或服务。每个服务都有一个用RPC-或者消息驱动API定义清楚的边界。微服务架构模式给采用单体式编码方式很难实现的功能提供了模块化的解决方案，由此，单个服务很容易开发、理解和维护。 第二，这种架构使得每个服务都可以有专门开发团队来开发。开发者可以自由选择开发技术，提供API服务。当然，许多公司试图避免混乱，只提供某些技术选择。然后，这种自由意味着开发者不需要被迫使用某项目开始时采用的过时技术，他们可以选择现在的技术。甚至于，因为服务都是相对简单，即使用现在技术重写以前代码也不是很困难的事情。 第三，微服务架构模式是每个微服务独立的部署。开发者不再需要协调其它服务部署对本服务的影响。这种改变可以加快部署速度。UI团队可以采用AB测试，快速的部署变化。微服务架构模式使得持续化部署成为可能。 最后，微服务架构模式使得每个服务独立扩展。你可以根据每个服务的规模来部署满足需求的规模。甚至于，你可以使用更适合于服务资源需求的硬件。比如，你可以在EC2 Compute Optimized instances上部署CPU敏感的服务，而在EC2 memory-optimized instances上部署内存数据库。 微服务架构的不足Fred Brooks在30年前写道，“there are no silver bullets”，像任何其它科技一样，微服务架构也有不足。其中一个跟他的名字类似，『微服务』强调了服务大小，实际上，有一些开发者鼓吹建立稍微大一些的，10-100 LOC服务组。尽管小服务更乐于被采用，但是不要忘了这只是终端的选择而不是最终的目的。微服务的目的是有效的拆分应用，实现敏捷开发和部署。 另外一个主要的不足是，微服务应用是分布式系统，由此会带来固有的复杂性。开发者需要在RPC或者消息传递之间选择并完成进程间通讯机制。更甚于，他们必须写代码来处理消息传递中速度过慢或者不可用等局部失效问题。当然这并不是什么难事，但相对于单体式应用中通过语言层级的方法或者进程调用，微服务下这种技术显得更复杂一些。 另外一个关于微服务的挑战来自于分区的数据库架构。商业交易中同时给多个业务分主体更新消息很普遍。这种交易对于单体式应用来说很容易，因为只有一个数据库。在微服务架构应用中，需要更新不同服务所使用的不同的数据库。使用分布式交易并不一定是好的选择，不仅仅是因为CAP理论，还因为今天高扩展性的NoSQL数据库和消息传递中间件并不支持这一需求。最终你不得不使用一个最终一致性的方法，从而对开发者提出了更高的要求和挑战。 测试一个基于微服务架构的应用也是很复杂的任务。比如，采用流行的Spring Boot架构，对一个单体式web应用，测试它的REST API，是很容易的事情。反过来，同样的服务测试需要启动和它有关的所有服务（至少需要这些服务的stubs）。再重申一次，不能低估了采用微服务架构带来的复杂性。 另外一个挑战在于，微服务架构模式应用的改变将会波及多个服务。比如，假设你在完成一个案例，需要修改服务A、B、C，而A依赖B，B依赖C。在单体式应用中，你只需要改变相关模块，整合变化，部署就好了。对比之下，微服务架构模式就需要考虑相关改变对不同服务的影响。比如，你需要更新服务C，然后是B，最后才是A，幸运的是，许多改变一般只影响一个服务，而需要协调多服务的改变很少。 部署一个微服务应用也很复杂，一个分布式应用只需要简单在复杂均衡器后面部署各自的服务器就好了。每个应用实例是需要配置诸如数据库和消息中间件等基础服务。相对比，一个微服务应用一般由大批服务构成。例如，根据Adrian Cockcroft，Hailo有160个不同服务构成，NetFlix有大约600个服务。每个服务都有多个实例。这就造成许多需要配置、部署、扩展和监控的部分，除此之外，你还需要完成一个服务发现机制（后续文章中发表），以用来发现与它通讯服务的地址（包括服务器地址和端口）。传统的解决问题办法不能用于解决这么复杂的问题。接续而来，成功部署一个微服务应用需要开发者有足够的控制部署方法，并高度自动化。 一种自动化方法是使用PaaS服务，例如Cloud Foundry。PaaS给开发者提供一个部署和管理微服务的简单方法，它把所有这些问题都打包内置解决了。同时，配置PaaS的系统和网络专家可以采用最佳实践和策略来简化这些问题。另外一个自动部署微服务应用的方法是开发对于你来说最基础的PaaS系统。一个典型的开始点是使用一个集群化方案，比如配合Docker使用Mesos或者Kubernetes。后面的系列我们会看看如何基于软件部署方法例如NGINX，可以方便的在微服务层面提供缓存、权限控制、API统计和监控。 总结构建复杂的应用真的是非常困难。单体式的架构更适合轻量级的简单应用。如果你用它来开发复杂应用，那真的会很糟糕。微服务架构模式可以用来构建复杂应用，当然，这种架构模型也有自己的缺点和挑战。 在后续的博客中，我会深入探索微服务架构模式，并讨论诸如服务发现、服务部署选择和如何分解一个分布式应用为多个服务的策略。 待续。。。。]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>SpringCloud</tag>
        <tag>单体应用</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【Java并发】JDK并发包之同步控制(6)]]></title>
    <url>%2F2017%2F11%2F01%2FJDK-concurrent-synchronization-control-6%2F</url>
    <content type="text"><![CDATA[摘要为了更好地支持并发程序，JDK内部提供了大量实用的API和框架。同步控制是并发编程必不可少的重要手段。 重入锁(RetreenLock)重入锁完全可以替代synchronized关键字，在JDK5.0，重入锁的性能要好于synchronized，在JDK6.0后synchronized做了大量的优化，两者性能差不多。 重入锁使用JDK自带的并发包java.util.concurrent.locks.ReentrantLock类实现。 代码实践12345678910111213141516171819202122232425262728public class RetreenLockThread implements Runnable &#123; public static ReentrantLock lock = new ReentrantLock(); public static int i = 0; @Override public void run() &#123; for (int j = 0; j &lt; 10000; j++) &#123; lock.lock(); try &#123; i++; &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; RetreenLockThread thread = new RetreenLockThread(); Thread t1 = new Thread(thread); Thread t2 = new Thread(thread); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println("结果：" + i); &#125;&#125; 上面的代码，我们使用重入锁保护临界区的资源i，确保多线程对i操作的安全。重入锁中，我们可以自己控制何时加锁何时释放锁，比synchronized来说更加灵活。使用重入锁，要保证最后一定要释放锁，否则其他线程就没办法方法临界区资源，从而进入死锁。 重入锁可以加多把锁，同时释放锁是否加几把锁就要释放几次。 中断响应对于synchronized来说，如果一个线程在等待锁，那么结果要么是继续等待，要么是获得锁继续执行。 锁申请等待限时除了等待外部通知外，要避免死锁就需要采用限时等待，即tryLock(long timeout, TimeUnit unit)。 公平锁在大多数情况下，锁都是非公平的。重入锁允许我们锁的公平性进行设置，通过构造函数进行public ReentrantLock(boolean fair)设置。当fair为true时表示锁是公平的。 正常需求情况是不推荐使用公平锁，因为实现成本高，性能低下。 综上，重入锁的实现包括三要素 原子状态 原子状态使用CAS操作来存储当前锁的状态，判断锁是否已经被别的线程持有。 等待队列 所有没有请求道锁的线程，都会进入等待队列进行等待。待有线程释放锁以后，系统就能够从等待队列中唤醒一个线程继续工作。 阻塞原语park()和unpark(),用来挂起和恢复线程。没有得到锁的线程会被挂起。 Condition条件Condition对象和Object.wait()及notify()方法作用类似。wait()和notify()方法是和synchronized关键字一起使用，而condtion是与重入锁相关联的。 Condition接口提供如下基本方法：1234567void await() throws InterruptedException;void awaitUninterruptibly();long awaitNanos(long nanosTimeout) throws InterruptedException;boolean await(long time, TimeUnit unit) throws InterruptedException;boolean awaitUntil(Date deadline) throws InterruptedException;void signal();void signalAll(); await()方法会使当前线程等待，同时释放当前锁，当其他线程中使用signal()或signalAll()时，线程会重新获得锁并继续执行。或者当线程被中断时也能跳出等待。 awaitUninterruptibly()和await()类似，但是不会再等待过程中响应中断。 signal()唤醒一个在等待中的线程，signalAll()唤醒所有在等待中的线程。 信号量(Semaphore)… 读写锁(ReadWriteLock)ReadWriteLock是JDK5中提供的读写分离锁。读写分离锁可以有效地帮助减少锁竞争，以提高系统性能。 读写锁允许多个线程同时读，但是考虑数据的完整性，写写操作和读写操作之间依然需要相互等待和持有锁。 倒计时器(CountDownLatch)CountDownLatch是通过一个计数器来实现的，计数器的初始值为线程的数量。每当一个线程完成了自己的任务后，计数器的值就会减1。当计数器值到达0时，它表示所有的线程已经完成了任务，然后在闭锁上等待的线程就可以恢复执行任务。 CountDownLatch的构造函数接收一个整数作为参数。1public void CountDownLatch(int count) &#123;...&#125; 构造器中的计数值（count）实际上就是闭锁需要等待的线程数量。这个值只能被设置一次，而且CountDownLatch没有提供任何机制去重新设置这个计数值。 与CountDownLatch的第一次交互是主线程等待其他线程。主线程必须在启动其他线程后立即调用CountDownLatch.await()方法。这样主线程的操作就会在这个方法上阻塞，直到其他线程完成各自的任务。 其他N 个线程必须引用闭锁对象，因为他们需要通知CountDownLatch对象，他们已经完成了各自的任务。这种通知机制是通过 CountDownLatch.countDown()方法来完成的；每调用一次这个方法，在构造函数中初始化的count值就减1。所以当N个线程都调 用了这个方法，count的值等于0，然后主线程就能通过await()方法，恢复执行自己的任务。 使用场景 实现最大的并行性：有时我们想同时启动多个线程，实现最大程度的并行性。例如，我们想测试一个单例类。如果我们创建一个初始计数为1的CountDownLatch，并让所有线程都在这个锁上等待，那么我们可以很轻松地完成测试。我们只需调用 一次countDown()方法就可以让所有的等待线程同时恢复执行。 开始执行前等待n个线程完成各自任务：例如应用程序启动类要确保在处理用户请求前，所有N个外部系统已经启动和运行了。 死锁检测：一个非常方便的使用场景是，你可以使用n个线程访问共享资源，在每次测试阶段的线程数目是不同的，并尝试产生死锁。 代码实践 … 循环栅栏(CyclicBarrier)CyclicBarrier是另外一种多线程并发控制实用工具，其功能和CountDownLatch类似，也可以实现线程间的计数等待。CyclicBarrier也可以理解为循环栅栏，计数器可以反复使用。 线程阻塞工具(LockSupport)LockSupport是个方便实用的线程阻塞工具，可以在线程内任意位置让线程阻塞。]]></content>
      <categories>
        <category>JAVA</category>
        <category>JAVA并发</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>JAVA并发</tag>
        <tag>多线程</tag>
        <tag>并发包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【Java并发】之高并发下ArrayList和HashMap运用(5)]]></title>
    <url>%2F2017%2F10%2F31%2FJava-Concurrent-ArrayList-And-HashMap-5%2F</url>
    <content type="text"><![CDATA[摘要在平常的java代码开发过程中，我们会经常性的就会用到ArrayList和HashMap来辅助自己完成需求工作，而且配合的也是相当的默契。但是如果环境发生改变，在并发情况下，他是否还能够顺利的完成我们想要的要求呢？ 并发下的ArrayList其实对于ArrayList而言，他并非是一个线程安全的容器，如果在多线程环境下使用ArrayList，必然会导致程序出错。 代码实践1234567891011121314151617181920212223242526public class ArrayListMultiThread &#123; static List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(10); public static class AddThread implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 1000000; i++) &#123; list.add(i); &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; // 创建两个线程t1,t2 Thread t1 = new Thread(new AddThread()); Thread t2 = new Thread(new AddThread());// 两个线程运行 t1.start(); t2.start();// 保持顺序输出 t1.join(); t2.join(); System.out.println(list.size()); &#125;&#125; t1,t2两个线程同时向ArrayList中添加容器，我们期望的效果肯定是2000000.但是由于线程不安全，它真的会得到我们想要的结果吗？ 情况一：由于ArrayList在扩容的过程中，内部一致性遭到了破坏，但是却没有任何工作去处理，没有得到锁的保护，使其另外一个线程看到了这样的局面，进而出现了越界的问题。最后的结果则出现了异常的情况。 情况二：两个线程同时访问，发生了肢体上的冲突，对容器的同一位置进行了同时刻的赋值，整体下来则出现了不一致的画面： 情况三：当然也有可能输出２００００００，得到我们想要的结果。每次都吵架，也会有和睦相处的时候啊！真心不易。 纵然有时候能够得到我们想要的结果的，但这也并非是我们想要的结果，唯一的解决方案就是更换一个“容器”，能够达到两者的永久性和睦相处，这样问题不就解决了。 我们采取更换的容器为：Vector 1static Vector&lt;Integer&gt; list = new Vector&lt;Integer&gt;(10); 更多Vector请查看后续博客（占坑中…） 并发下的HashMapHashMap相比ArrayList而言，也是线程不安全的。 代码实践1234567891011121314151617181920212223242526272829303132public class HashMapMultiThread &#123; // 定义一个HashMap集合 static Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); public static class AddThread implements Runnable &#123; int start = 0; public AddThread(int start) &#123; this.start = start; &#125; @Override public void run() &#123; //对i进行+2操作 for (int i = start; i &lt; 100000; i += 2) &#123; //赋值以2为底的无符号整数 map.put(Integer.toString(i), Integer.toBinaryString(i)); &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; //创建两个线程t1,t2 Thread t1 = new Thread(new HashMapMultiThread.AddThread(0)); Thread t2 = new Thread(new HashMapMultiThread.AddThread(1)); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(map.size()); &#125;&#125; 上面的代码，我们预期的结果是输出100000，但是实际情况有三种： 程序正常结束，结果符合预期。 程序正常结束，结果不符合预期，size大小不满足100000 程序无法结束。 为了避免进程不安全，在并发的情况下，我们可以使用ConcurrentHashMap来代替HashMap工作。 1static Map&lt;String, String&gt; map = new ConcurrentHashMap&lt;&gt;(); 在并发的环境下，其实有些操作都是可以避免的，比如一些线程不安全我们完全可以用线程安全的去取代其进行工作。]]></content>
      <categories>
        <category>JAVA</category>
        <category>JAVA并发</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>JAVA并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【Java并发】之线程控制(4)]]></title>
    <url>%2F2017%2F10%2F31%2FJava-multithreading-based-on-concurrent-4%2F</url>
    <content type="text"><![CDATA[摘要Java的线程支持提供了一些便捷的工具方法，通过这些便捷的工具方法可以很好地控制线程的执行 join线程控制，让一个线程等待另一个线程完成的方法 后台线程，又称为守护线程或精灵线程。它的任务是为其他的线程提供服务，如果所有的前台线程都死亡，后台线程会自动死亡 线程睡眠sleep，让当前正在执行的线程暂停一段时，并进入阻塞状态 线程让步yield，让当前正在执行的线程暂停，但它不会阻塞该线程，它只是将该线程转入就绪状态 join线程Thread提供了让一个线程等待另一个线程完成的方法join()方法。当在某个程序执行流中调用其他线程的join()方法时，调用线程将被阻塞，直到被join()方法加入的join线程执行完为止。join()方法通常由使用线程的程序调用，以将大问题划分成许多小问题，每个小问题分配一个线程。当所有的小问题都得到处理后，再调用主线程来进一步操作。 实践12345678910111213141516171819202122232425262728293031323334353637public class JoinThread &#123; public static class CurrentThread implements Runnable &#123; @Override public void run() &#123; System.out.println("开始当前线程：" + ZonedDateTime.now()); &#125; &#125; public static class NewThread implements Runnable &#123; @Override public void run() &#123; System.out.println("你要等我执行完毕" + ZonedDateTime.now()); &#125; &#125; public static void main(String[] args) &#123; new Thread(new CurrentThread()).start(); for (int i = 0; i &lt; 10; i++) &#123; if (i == 5) &#123; Thread newThread = new Thread(new NewThread()); // main线程调用了jt线程的join()方法，main线程 // 必须等jt执行结束才会向下执行 try &#123; newThread.start(); newThread.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(Thread.currentThread().getName() + "" + i); &#125; &#125;&#125; 结果：123456789101112main0main1main2main3main4你要等我执行完毕2017-10-31T22:16:33.759+08:00[Asia/Shanghai]开始当前线程：2017-10-31T22:16:33.759+08:00[Asia/Shanghai]main5main6main7main8main9 上面有三个线程,当主线程执行到i=5的时候，启动了一个新的线程NewThread，该线程不会和main线程并发执行，main线程在此刻必须等待新线程执行结束后才会继续执行。在新线程执行的时候，main实际上就处于等待状态。 后台线程（守护线程）有一种线程，它是在后台运行的，它的任务是为其他的线程提供服务，这种线程被称为后台线程（Daemon Thread），又称为守护线程或精灵线程。JVM的垃圾回收线程就是典型的后台线程。后台线程有个特征：如果所有的前台线程都死亡，后台线程会自动死亡。 调用Thread对象的setDaemon(true)方法可将指定线程设置成后台线程。下面程序将执行线程设置成后台线程，可以看到当所有的前台线程死亡时，后台线程随之死亡。当整个虚拟机中只剩下后台线程时，程序就没有继续运行的必要了，所以虚拟机也就退出了。 代码实践123456789101112131415161718192021public class DaemonThread extends Thread &#123; // 定义后台线程的线程执行体与普通线程没有任何区别 public void run() &#123; for (int i = 0; i &lt; 1000; i++) &#123; System.out.println(getName() + "" + i); &#125; &#125; public static void main(String[] args) &#123; DaemonThread t = new DaemonThread(); // 将此线程设置成后台线程 t.setDaemon(true); // 启动后台线程 t.start(); for (int i = 0; i &lt; 10; i++) &#123; System.out.println(Thread.currentThread().getName() + "" + i); &#125; // -----程序执行到此处，前台线程（main线程）结束------ // 后台线程也应该随之结束 &#125;&#125; 上面程序中的t线程设置成后台线程，然后启动该线程，本来该线程应该执行到i等于999时才会结束，但运行程序时不难发现该后台线程无法运行到999，因为当主线程也就是程序中唯一的前台线程运行结束后，JVM会主动退出，因而后台线程也就被结束了。Thread类还提供了一个isDaemon0方法，用于判断指定线程是否为后台线程 从上面程序可以看出，主线程默认是前台线程， t线程默认也是前台线程。并不是所有的线程默认都是前台线程，有些线程默认就是后台线程——前台线程创建的子线程默认是前台线程，后台线程创建的子线程默认是后台线程 前台线程死亡后，JVM会通知后台线程死亡，但从它接收指令到做出响应，需要一定时间。而且要将某个线程设置为后台线程，必须在该线程启动之前设置，也就是说setDaemon(true)必须在start()方法之前调用，否则会引发IllegaIThreadStateException异常。 线程睡眠（sleep）如果需要让当前正在执行的线程暂停一段时，并进入阻塞状态，则可以通过调用Thread类的静态sleep()方法来实现。当当前线程调用sleep()方法进入阻塞状态后，在其睡眠时间段内，该线程不会获得执行的机会，即使系统中没有其他可执行的线程，处于sleep()中的线程也不会执行，因此sleep()方法常用来暂停程序的执行。 下面程序调用sleep()方法来暂停主线程的执行，因为该程序只有一个主线程，当主线程进入睡眠后，系统没有可执行的线程，所以可以看到程序在sleep()方法处暂停 线程让步（yield）yield()方法是一个和sleep()方法有点相似的方法，它也是Thread类提供的一个静态方法，它也可以让当前正在执行的线程暂停，但它不会阻塞该线程，它只是将该线程转入就绪状态。yield()只是让当前线程暂停一下，让系统的线程调度器重新调度一次，完全可能的情况是：当某个线程调用了yield()方法暂停之后，线程调度器又将其调度出来重新执行。 实际上，当某个线程调用了yield()方法暂停之后，只有优先级与当前线程相同，或者优先级比当前线程更高的处于就绪状态的线程才会获得执行的机会。下面程序使用yield()方法来让当前正在执行的线程暂停。 sleep()方法和yield()方法的区别 sleep()方法暂停当前线程后，会给其他线程执行机会，不会理会其他线程的优先级；但yield()方法只会给优先级相同，或优先级更高的线程执行机会 sleep()方法会将线程转入阻塞状态，直到经过阻塞时间才会转入就绪状态：而yield()不会将线程转入阻塞状态，它只是强制当前线程进入就绪状态。因此完全有可能某个线程调用yield()方法暂停之后，立即再次获得处理器资源被执行 sleep()方法声明抛出了InterruptcdException异常，所以调用sleep()方法时要么捕捉该异常，要么显式声明抛出该异常；而yield()方法则没有声明抛出任何异常 sleep()方法比yield()方法有更好的可移植性，通常不建议使用yield()方法来控制并发线程的执行]]></content>
      <categories>
        <category>JAVA</category>
        <category>JAVA并发</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>JAVA并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【Java并发】之多线程wait()和notify()(3)]]></title>
    <url>%2F2017%2F10%2F31%2FJava-multithreading-based-on-concurrent-3%2F</url>
    <content type="text"><![CDATA[概要在多线程的情况下，由于同一进程的多个线程共享同一片存储空间，在带来方便的同时，也带来了访问冲突这个严重的问题。Java语言提供了专门机制以解决这种冲突，有效避免了同一个数据对象被多个线程同时访问。 为了支持多线程之间的协作，JDK提供了2个非常重要的接口线程等待wait()方法和通知notify()方法。结合与synchronized关键字使用，可以建立很多优秀的同步模型。 同步分为类级别和对象级别，分别对应着类锁和对象锁。类锁是每个类只有一个，如果static的方法被synchronized关键字修饰，则在这个方法被执行前必须获得类锁； synchronized中使用首先，调用一个Object的wait与notify/notifyAll的时候，必须保证调用代码对该Object是同步的，也就是说必须在作用等同于1synchronized(obj)&#123;......&#125; 的内部才能够去调用obj的wait与notify/notifyAll三个方法，否则就会报错：1java.lang.IllegalMonitorStateException:current thread not owner 在调用wait的时候，线程自动释放其占有的对象锁，同时不会去申请对象锁。当线程被唤醒的时候，它才再次获得了去获得对象锁的权利。 notify与notifyAll没有太多的区别，只是notify仅唤醒一个线程并允许它去获得锁，notifyAll是唤醒所有等待这个对象的线程并允许它们去获得对象锁，只要是在synchronied块中的代码，没有对象锁是寸步难行的。其实唤醒一个线程就是重新允许这个线程去获得对象锁并向下运行。 notifyAll，虽然是对每个wait的对象都调用一次notify，但是这个还是有顺序的，每个对象都保存这一个等待对象链，调用的顺序就是这个链的顺序。其实启动等待对象链中各个线程的也是一个线程，在具体应用的时候，需要注意一下。 wait()，notify()，notifyAll()不属于Thread类,而是属于Object基础类,也就是说每个对像都有wait()，notify()，notifyAll()的功能。因为都个对像都有锁,锁是每个对像的基础,当然操作锁的方法也是最基础了。 实践案例123456789101112131415161718192021222324252627282930313233343536373839404142public class SimpleWatiAndNotify &#123; private static final Object object = new Object(); public static class ThreadOne implements Runnable &#123; @Override public void run() &#123; synchronized (object) &#123; System.out.println("线程1启动===&gt;" + ZonedDateTime.now()); try &#123; object.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("线程1结束===&gt;" + ZonedDateTime.now()); &#125; &#125; &#125; public static class ThreadTwo implements Runnable &#123; @Override public void run() &#123; synchronized (object) &#123; System.out.println("线程2启动===&gt;" + ZonedDateTime.now()); // 对象消息通知 object.notify(); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("线程2结束===&gt;" + ZonedDateTime.now()); &#125; &#125; &#125; public static void main(String[] args) &#123; Thread one = new Thread(new ThreadOne()); Thread two = new Thread(new ThreadTwo()); one.start(); two.start(); &#125;&#125; 执行结果：1234线程1启动===&gt;2017-10-31T21:24:37.734+08:00[Asia/Shanghai]线程2启动===&gt;2017-10-31T21:24:37.735+08:00[Asia/Shanghai]线程2结束===&gt;2017-10-31T21:24:40.735+08:00[Asia/Shanghai]线程1结束===&gt;2017-10-31T21:24:40.735+08:00[Asia/Shanghai] 通过上面代码,我们开启线程1和线程2.其中线程1执行了object.wait()方法，即线程1申请了object的对象锁。那么执行完wait()方法后，线程1就就会进入等待状态，并释放object的锁。线程2在执行notify()方法之前也会先获得object的对象锁。线程1和线程2的结束时间差3S，符合我们的预期。 我们也发现线程2通知线程1后，线程1并没有立即执行，而是在等待线程2释放object的锁,并重写获取到锁之后才会继续执行。 问题1：如果线程2中的 object.notify();注释掉会发生什么。 123线程1启动===&gt;2017-10-31T21:33:04.259+08:00[Asia/Shanghai]线程2启动===&gt;2017-10-31T21:33:04.261+08:00[Asia/Shanghai]线程2结束===&gt;2017-10-31T21:33:07.261+08:00[Asia/Shanghai] 查看执行结果，我们发现线程2都已经执行完毕，已经释放object的锁，但是线程1依旧在等待中。这样就已经进入死锁状态了。 notify方法很容易引起死锁，除非你根据自己的程序设计，确定不会发生死锁，notifyAll方法则是线程的安全唤醒方法。]]></content>
      <categories>
        <category>JAVA</category>
        <category>JAVA并发</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>JAVA并发</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【Java并发】之多线程基础(2)]]></title>
    <url>%2F2017%2F10%2F30%2FJava-multithreading-based-on-concurrent-2%2F</url>
    <content type="text"><![CDATA[进程和线程进程是指处于运行过程中的程序，并且具有一定的独立性。进程是系统进行资源分配和调度的一个单位。当程序进入内存运行时，即为进程。 进程特点 独立性 进程是系统中独立存在的实体，它可以独立拥有资源，每一个进程都有自己独立的地址空间，没有进程本身的运行，用户进程不可以直接访问其他进程的地址空间。 动态性 进程和程序的区别在于进程是动态的，进程中有时间的概念，进程具有自己的生命周期和各种不同的状态。 并发性 多个进程可以在单个处理器上并发执行，互不影响。 并行和并发区别：并行：同一时刻可以处理多个任务 并发：统一时刻只能处理1个任务，但在一段时间内可以对多个任务交替处理 图示说明： 线程特点线程是进程的组成部分，一个进程可以拥有多个线程，而一个线程必须拥有一个父进程。线程可以拥有自己的堆栈，自己的程序计数器和自己的局部变量，但不能拥有系统资源。它与父进程的其他线程共享该进程的所有资源。 线程可以完成一定任务，可以和其它线程共享父进程的共享变量和部分环境，相互协作来完成任务。 线程是独立运行的，其不知道进程中是否还有其他线程存在。 线程的执行是抢占式的，也就是说，当前执行的线程随时可能被挂起，以便运行另一个线程。 一个线程可以创建或撤销另一个线程，一个进程中的多个线程可以并发执行。 线程生命周期当线程被创建并启动以后，它既不是一启动就进入了执行状态，也不是一直处于执行状态。在线程的生命周期中，它要经过新建(New)、就绪（Runnable）、运行（Running）、阻塞(Blocked)和死亡(Dead)5种状态。尤其是当线程启动以后，它不可能一直”霸占”着CPU独自运行，所以CPU需要在多条线程之间切换，于是线程状态也会多次在运行、阻塞之间切换。 新建状态，当程序使用new关键字创建了一个线程之后，该线程就处于新建状态，此时仅由JVM为其分配内存，并初始化其成员变量的值。 就绪状态，当线程对象调用了start()方法之后，该线程处于就绪状态。Java虚拟机会为其创建方法调用栈和程序计数器，等待调度运行。 运行状态，如果处于就绪状态的线程获得了CPU，开始执行run()方法的线程执行体，则该线程处于运行状态 阻塞状态，当处于运行状态的线程失去所占用资源之后，便进入阻塞状态 在线程的生命周期当中，线程的各种状态的转换过程 新建和就绪状态当程序使用new关键字创建了一个线程之后，该线程就处于新建状态，此时它和其他的Java对象一样，仅仅由Java虚拟机为其分配内存，并初始化其成员变量的值。此时的线程对象没有表现出任何线程的动态特征，程序也不会执行线程的线程执行体。 当线程对象调用了start()方法之后，该线程处于就绪状态。Java虚拟机会为其创建方法调用栈和程序计数器，处于这个状态中的线程并没有开始运行，只是表示该线程可以运行了。至于该线程何时开始运行，取决于JVM里线程调度器的调度。 调用线程对象的start()方法之后，该线程立即进入就绪状态——就绪状态相当于”等待执行”，但该线程并未真正进入运行状态。如果希望调用子线程的start()方法后子线程立即开始执行，程序可以使用Thread.sleep(1) 来让当前运行的线程（主线程）睡眠1毫秒，1毫秒就够了，因为在这1毫秒内CPU不会空闲，它会去执行另一个处于就绪状态的线程，这样就可以让子线程立即开始执行。 注意：启动线程使用start()方法，而不是run()方法。永远不要调用线程对象的run()方法。调用start0方法来启动线程，系统会把该run()方法当成线程执行体来处理；但如果直按调用线程对象的run()方法，则run()方法立即就会被执行，而且在run()方法返回之前其他线程无法并发执行。也就是说，系统把线程对象当成一个普通对象，而run()方法也是一个普通方法，而不是线程执行体。需要指出的是，调用了线程的run()方法之后，该线程已经不再处于新建状态，不要再次调用线程对象的start()方法。只能对处于新建状态的线程调用start()方法，否则将引发IllegaIThreadStateExccption异常。 运行和阻塞状态线程调度如果处于就绪状态的线程获得了CPU，开始执行run()方法的线程执行体，则该线程处于运行状态，如果计算机只有一个CPU。那么在任何时刻只有一个线程处于运行状态，当然在一个多处理器的机器上，将会有多个线程并行执行；当线程数大于处理器数时，依然会存在多个线程在同一个CPU上轮换的现象。 当一个线程开始运行后，它不可能一直处于运行状态（除非它的线程执行体足够短，瞬间就执行结束了）。线程在运行过程中需要被中断，目的是使其他线程获得执行的机会，线程调度的细节取决于底层平台所采用的策略。对于采用抢占式策略的系统而言，系统会给每个可执行的线程一个小时间段来处理任务；当该时间段用完后，系统就会剥夺该线程所占用的资源，让其他线程获得执行的机会。在选择下一个线程时，系统会考虑线程的优先级。 所有现代的桌面和服务器操作系统都采用抢占式调度策略，但一些小型设备如手机则可能采用协作式调度策略，在这样的系统中，只有当一个线程调用了它的sleep()或yield()方法后才会放弃所占用的资源——也就是必须由该线程主动放弃所占用的资源。 线程中断线程中断时一种重要的线程协作机制。线程中断并不会使线程立即退出，而是给线程发送一个通知，告知目标线程，有人希望你退出，至于线程接到通知如何处理，完成是线程自行决定。 JDK中默认提供了强大的支持。 123Thread.interrupt(); // 中断线程Thread.isInterrupted(); // 判断是否被中断Thread.isTerrupted(); // 判断是否被中断，并清除当前中断状态 线程阻塞当发生如下情况时，线程将会进入阻塞状态 线程调用sleep()方法主动放弃所占用的处理器资源 线程调用了一个阻塞式IO方法，在该方法返回之前，该线程被阻塞 线程试图获得一个同步监视器，但该同步监视器正被其他线程所持有。关于同步监视器的知识、后面将存更深入的介绍 线程在等待某个通知（notify） 程序调用了线程的suspend()方法将该线程挂起。但这个方法容易导致死锁，所以应该尽量避免使用该方法 当前正在执行的线程被阻塞之后，其他线程就可以获得执行的机会。被阻塞的线程会在合适的时候重新进入就绪状态，注意是就绪状态而不是运行状态。也就是说，被阻塞线程的阻塞解除后，必须重新等待线程调度器再次调度它。 解除阻塞针对上面几种情况，当发生如下特定的情况时可以解除上面的阻塞，让该线程重新进入就绪状态： 调用sleep()方法的线程经过了指定时间。 线程调用的阻塞式IO方法已经返回。 线程成功地获得了试图取得的同步监视器。 线程正在等待某个通知时，其他线程发出了个通知。 处于挂起状态的线程被调甩了resdme()恢复方法。 线程整个周期状态图如下： 从图中可以看出，线程从阻塞状态只能进入就绪状态，无法直接进入运行状态。而就绪和运行状态之间的转换通常不受程序控制，而是由系统线程调度所决定。当处于就绪状态的线程获得处理器资源时，该线程进入运行状态；当处于运行状态的线程失去处理器资源时，该线程进入就绪状态。但有一个方法例外，调用yield()方法可以让运行状态的线程转入就绪状态。 线程死亡死亡状态线程会以如下3种方式结束，结束后就处于死亡状态： run()或call()方法执行完成，线程正常结束。 线程抛出一个未捕获的Exception或Error。 直接调用该线程stop()方法来结束该线程——该方法容易导致死锁，通常不推荐使用。常用方式是通过标识方式优雅结束线程。 程序设计当主线程结束时，其他线程不受任何影响，并不会随之结束。一旦子线程启动起来后，它就拥有和主线程相同的地位，它不会受主线程的影响。为了测试某个线程是否已经死亡，可以调用线程对象的isAlivc()方法，当线程处于就绪、运行、阻塞了种状态时，该方法将返回true；当线程处于新建、死亡状态时，该方法将返回false。 不要试图对一个已经死亡的线程调用start()方法使它重新启动，死亡就是死亡，该线程将不可再次作为线程执行。 参考资料：http://www.cnblogs.com/sunddenly/p/4106562.html]]></content>
      <categories>
        <category>JAVA</category>
        <category>JAVA并发</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>JAVA并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【Java并发】之多线程基础(1)]]></title>
    <url>%2F2017%2F10%2F30%2FJava-multithreading-based-on-concurrent-1%2F</url>
    <content type="text"><![CDATA[简介在传统的线程技术中，创建多线程有2中方式 继承Thread类，并重写run()方法； 实现Runnable接口，覆盖接口的run()方法； 探究Thread类的run()源码打开Thread类的定义，我们发现run()的实现如下： 123456@Override public void run() &#123; if (target != null) &#123; target.run(); &#125; &#125; 我们发现run()的实现方法很简单，就是一个简单的if判断语句。那么target又是何方神圣呢？我们通过IDEA跟踪到taget，发现如下： 1private Runnable target; 查看源码我们发现，原来target其实就是一个类型为Runnable接口的对象，且是通过构造器传入的。 再点击Runnable进去，我们又发现新大陆：1234@FunctionalInterfacepublic interface Runnable &#123; public abstract void run();&#125; Runnable接口中只有1个抽象run()方法。 通过上面的源码查看分析，我们可以知道Thread类中的target如果不为空，就会实现Runnable接口的run()，那么我们执行的run()方法就是Runnable中的方法。 根据上面的源码，我们也就很容易知道为什么创建多线程有2种方式了。 通过继承Thread类实现多线程 继承Thread类，并实现run()方法 调用start()方法开启线程 为了简单测试，我们直接在main方法中通过内部类测试12345678910111213141516public class TraditionalThread &#123; public static void main(String[] args) &#123; Thread thread = new Thread() &#123; @Override public void run() &#123; try &#123; Thread.sleep(500);//让线程休息500毫秒 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName());//打印出当前线程名 &#125; &#125;; thread.start(); &#125;&#125; 通过实现Runnable接口实现多线程 实现Runnable接口，并实现run()方法 调用start()方法开启线程 12345678910111213141516public class TraditionalThread &#123; public static void main(String[] args) &#123; Thread thread = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()); &#125; &#125;); thread.start(); &#125;&#125;]]></content>
      <categories>
        <category>JAVA</category>
        <category>JAVA并发</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>JAVA并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA虚拟机结构]]></title>
    <url>%2F2017%2F10%2F11%2FThe-Structure-of-the-Java-Virtual-Machine%2F</url>
    <content type="text"><![CDATA[class文件格式class文件是一种8位字节的二进制流文件，各个数据项按顺序紧密的从前向后排列，相邻的项之间没有间隙，使得class文件非常紧凑，体积轻巧，可以被JVM快速加载到内存，并且占据较少的内存空间。JAVA源文件被编译后，每个类或者接口都单独占据1个class文件，并且类中的所有信息都会在class文件中有相应的描述。 class文件中的信息是一项一项排列的，每项数据都有它的固定长度，或占1个字节，或2个字节，或4个字节，或8个字节。数据项的长度分别用u1，u2，u4，u8表。 详细介绍:(后面补充) 数据类型Java虚拟机可以操作的数据类型分为2类，原始类型和引用类型。也存在原始值和引用值，用于变量赋值，参数传递，方法方法和运算操作。 JAVA虚拟机希望尽可能多的类型检查在程序运行前完成，即编译器在编译期间就要尽最大努力完成可能的类型检查，使虚拟机在运行期间无需这些操作。 原始类型：primitive type数值类型 整型类型 byte类型： 值为8位有符号二进制补码整数，默认值0，取值范围 -128 ~ 127 short类型：值为16位有符号二进制补码整数，默认值0，取值范围 -32768 ~ 32767 int类型： 值为32位有符号二进制补码整数，默认值0，取值范围 -2147483648 ~ 2147483647 long类型： 值为64位有符号二进制补码整数，默认值0，取值范围 -9223372036854775808 ~ 9223372036854775808 char类型： 值为用16位无符号整数表示，，取值范围 0 ~ 65535 浮点类型 float类型； 值为单精度浮点数集合中的元素，默认值为正数0. double类型：值为双精度浮点数集合中的元素，默认值为正数0. 布尔类型 值为true和false，默认值false。 returnAddress类型 引用类型：reference type运行时数据区栈帧对象的表示浮点算法特殊方法异常字节码指令集简介类库共有设计、私有实现]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>虚拟机</tag>
        <tag>JVM</tag>
        <tag>JAVA8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA虚拟机规范 (JAVA SE 8 Edition)]]></title>
    <url>%2F2017%2F10%2F11%2FThe-Java-Virtual-Machine-Specification-JAVA-SE-8-Edition%2F</url>
    <content type="text"><![CDATA[前言JAVA9已经发布了，阿里达摩院也横空出世了，在不学习，就跟不上年轻的步伐，就跟不上时代的潮流。作为一个6年的JAVA开发者，是时候深入研究学习。本系列博客，仅作为学习笔记。 JAVA简史Java是一种计算机编程语言，拥有跨平台、面向对象、泛型编程的特性，广泛应用于企业级Web应用开发和移动应用开发。 1991年4月，由James Gosling博士领导的绿色计划（Green Project）开始启动，此计划的目的是开发一种能够在各种消费性电子产品（如机顶盒、冰箱、收音机等）上运行的程序架构。这个计划的产品就是Java语言的前身：Oak（橡树）。Oak当时在消费品市场上并不算成功，但随着1995年互联网潮流的兴起，Oak迅速找到了最适合自己发展的市场定位并蜕变成为Java语言。 1992年3月，由于Oak已被用作另一种已存在的编程语言名称，因此必须选一个新的名字——它就是Java，灵感来源于咖啡。 1993年2月，电视机顶盒，FirstPerson试图从时代华纳获得一个电视机顶盒交互系统的一揽子订单。在那时，由于绿色计划不是很成功，随即失去了时代华纳的订单。于是开发的重心从家庭消费电子产品转到了电视盒机顶盒的相关平台上。 1995年5月23日，Oak语言改名为Java，并且在SunWorld大会上正式发布Java 1.0版本。Java语言第一次提出了“Write Once，Run Anywhere”的口号。 1996年1月23日，JDK 1.0发布，Java语言有了第一个正式版本的运行环境。JDK 1.0提供了一个纯解释执行的Java虚拟机实现（Sun Classic VM）。JDK 1.0版本的代表技术包括：Java虚拟机、Applet、AWT等。 1996年4月，10个最主要的操作系统供应商申明将在其产品中嵌入Java技术。同年9月，已有大约8.3万个网页应用了Java技术来制作。在1996年5月底，Sun公司于美国旧金山举行了首届JavaOne大会，从此JavaOne成为全世界数百万Java语言开发者每年一度的技术盛会。 1997年2月19日，Sun公司发布了JDK 1.1，Java技术的一些最基础的支撑点（如JDBC等）都是在JDK 1.1版本中发布的，JDK 1.1版的技术代表有：JAR文件格式、JDBC、JavaBeans、RMI。Java语法也有了一定的发展，如内部类（Inner Class）和反射（Reflection）都是在这个时候出现的。 直到1999年4月8日，JDK 1.1一共发布了1.1.0～1.1.8九个版本。从1.1.4之后，每个JDK版本都有一个自己的名字（工程代号），分别为：JDK 1.1.4 - Sparkler（宝石）、JDK 1.1.5 - Pumpkin（南瓜）、JDK 1.1.6 - Abigail（阿比盖尔，女子名）、JDK 1.1.7 - Brutus（布鲁图，古罗马政治家和将军）和JDK 1.1.8 – Chelsea（切尔西，城市名）。 1998年12月4日，JDK迎来了一个里程碑式的版本JDK 1.2，工程代号为Playground（竞技场），Sun在这个版本中把Java技术体系拆分为3个方向，分别是面向桌面应用开发的J2SE（Java 2 Platform， Standard Edition）、面向企业级开发的J2EE（Java 2 Platform， Enterprise Edition）和面向手机等移动终端开发的J2ME（Java 2 Platform， Micro Edition）。在这个版本中出现的代表性技术非常多，如EJB、Java Plug-in、Java IDL、Swing等，并且这个版本中Java虚拟机第一次内置了JIT（Just In Time）编译器（JDK 1.2中曾并存过3个虚拟机，Classic VM、HotSpot VM和Exact VM，其中Exact VM只在Solaris平台出现过；后面两个虚拟机都是内置JIT编译器的，而之前版本所带的Classic VM只能以外挂的形式使用JIT编译器）。在语言和API级别上，Java添加了strictfp关键字与现在Java编码之中极为常用的一系列Collections集合类。 在1999年3月和7月，分别有JDK 1.2.1和JDK 1.2.2两个小版本发布。 1999年4月27日，HotSpot虚拟机发布，HotSpot最初由一家名为“Longview Technologies”的小公司开发，因为HotSpot的优异表现，这家公司在1997年被Sun公司收购了。HotSpot虚拟机发布时是作为JDK 1.2的附加程序提供的，后来它成为了JDK 1.3及之后所有版本的Sun JDK的默认虚拟机。 2000年5月8日，工程代号为Kestrel（美洲红隼）的JDK 1.3发布，JDK 1.3相对于JDK 1.2的改进主要表现在一些类库上（如数学运算和新的Timer API等），JNDI服务从JDK 1.3开始被作为一项平台级服务提供（以前JNDI仅仅是一项扩展），使用CORBA IIOP来实现RMI的通信协议，等等。这个版本还对Java 2D做了很多改进，提供了大量新的Java 2D API，并且新添加了JavaSound类库。JDK 1.3有1个修正版本JDK 1.3.1，工程代号为Ladybird（瓢虫），于2001年5月17日发布。 自从JDK 1.3开始，Sun维持了一个习惯：大约每隔两年发布一个JDK的主版本，以动物命名，期间发布的各个修正版本则以昆虫作为工程名称。 2002年2月13日，JDK 1.4发布，工程代号为Merlin（灰背隼）。JDK 1.4是Java真正走向成熟的一个版本，Compaq、Fujitsu、SAS、Symbian、IBM等著名公司都有参与甚至实现自己独立的JDK 1.4。哪怕是在十多年后的今天，仍然有许多主流应用（Spring、Hibernate、Struts等）能直接运行在JDK 1.4之上，或者继续发布能运行在JDK 1.4上的版本。JDK 1.4同样发布了很多新的技术特性，如正则表达式、异常链、NIO、日志类、XML解析器和XSLT转换器等。 JDK 1.4有两个后续修正版： 2002年9月16日发布的工程代号为Grasshopper（蚱蜢）的JDK 1.4.1 2003年6月26日发布的工程代号为Mantis（螳螂）的JDK 1.4.2。 2002年前后还发生了一件与Java没有直接关系，但事实上对Java的发展进程影响很大的事件，那就是微软公司的.NET Framework发布了。这个无论是技术实现上还是目标用户上都与Java有很多相近之处的技术平台给Java带来了很多讨论、比较和竞争，.NET平台和Java平台之间声势浩大的孰优孰劣的论战到目前为止都在继续。 2004年9月30日，JDK 1.5发布，工程代号Tiger（老虎）。从JDK 1.2以来，Java在语法层面上的变换一直很小，而JDK 1.5在Java语法易用性上做出了非常大的改进。例如，自动装箱、泛型、动态注解、枚举、可变长参数、遍历循环（foreach循环）等语法特性都是在JDK 1.5中加入的。在虚拟机和API层面上，这个版本改进了Java的内存模型（Java Memory Model，JMM）、提供了java.util.concurrent并发包等。另外，JDK 1.5是官方声明可以支持Windows 9x平台的最后一个JDK版本。 2006年12月11日，JDK 1.6发布，工程代号Mustang（野马）。在这个版本中，Sun终结了从JDK 1.2开始已经有8年历史的J2EE、J2SE、J2ME的命名方式，启用Java SE 6、Java EE 6、Java ME 6的命名方式。JDK 1.6的改进包括：提供动态语言支持（通过内置Mozilla Java Rhino引擎实现）、提供编译API和微型HTTP服务器API等。同时，这个版本对Java虚拟机内部做了大量改进，包括锁与同步、垃圾收集、类加载等方面的算法都有相当多的改动。 在2006年11月13日的JavaOne大会上，Sun公司宣布最终会将Java开源，并在随后的一年多时间内，陆续将JDK的各个部分在GPL v2（GNU General Public License v2）协议下公开了源码，并建立了OpenJDK组织对这些源码进行独立管理。除了极少量的产权代码（Encumbered Code，这部分代码大多是Sun本身也无权限进行开源处理的）外，OpenJDK几乎包括了Sun JDK的全部代码，OpenJDK的质量主管曾经表示，在JDK 1.7中，Sun JDK和OpenJDK除了代码文件头的版权注释之外，代码基本上完全一样，所以OpenJDK 7与Sun JDK 1.7本质上就是同一套代码库开发的产品。 JDK 1.6发布以后，由于代码复杂性的增加、JDK开源、开发JavaFX、经济危机及Sun收购案等原因，Sun在JDK发展以外的事情上耗费了很多资源，JDK的更新没有再维持两年发布一个主版本的发展速度。JDK 1.6到目前为止一共发布了37个Update版本，最新的版本为Java SE 6 Update 37，于2012年10月16日发布。 2009年2月19日，工程代号为Dolphin（海豚）的JDK 1.7完成了其第一个里程碑版本。根据JDK 1.7的功能规划，一共设置了10个里程碑。最后一个里程碑版本原计划于2010年9月9日结束，但由于各种原因，JDK 1.7最终无法按计划完成。 从JDK 1.7最开始的功能规划来看，它本应是一个包含许多重要改进的JDK版本，其中的Lambda项目（Lambda表达式、函数式编程）、Jigsaw项目（虚拟机模块化支持）、动态语言支持、GarbageFirst收集器和Coin项目（语言细节进化）等子项目对于Java业界都会产生深远的影响。在JDK 1.7开发期间，Sun公司由于相继在技术竞争和商业竞争中都陷入泥潭，公司的股票市值跌至仅有高峰时期的3%，已无力推动JDK 1.7的研发工作按正常计划进行。为了尽快结束JDK 1.7长期“跳票”的问题，Oracle公司收购Sun公司后不久便宣布将实行“B计划”，大幅裁剪了JDK 1.7预定目标，以便保证JDK 1.7的正式版能够于2011年7月28日准时发布。“B计划”把不能按时完成的Lambda项目、Jigsaw项目和Coin项目的部分改进延迟到JDK 1.8之中。最终，JDK 1.7的主要改进包括：提供新的G1收集器（G1在发布时依然处于Experimental状态，直至2012年4月的Update 4中才正式“转正”）、加强对非Java语言的调用支持（JSR-292，这项特性到目前为止依然没有完全实现定型）、升级类加载架构等。 到目前为止，JDK 1.7已经发布了9个Update版本，最新的Java SE 7 Update 9于2012年10月16日发布。从Java SE 7 Update 4起，Oracle开始支持Mac OS X操作系统，并在Update 6中达到完全支持的程度，同时，在Update 6中还对ARM指令集架构提供了支持。至此，官方提供的JDK可以运行于Windows（不含Windows 9x）、Linux、Solaris和Mac OS平台上，支持ARM、x86、x64和Sparc指令集架构类型。 2009年4月20日，Oracle公司宣布正式以74亿美元的价格收购Sun公司，Java商标从此正式归Oracle所有（Java语言本身并不属于哪间公司所有，它由JCP组织进行管理，尽管JCP主要是由Sun公司或者说Oracle公司所领导的）。由于此前Oracle公司已经收购了另外一家大型的中间件企业BEA公司，在完成对Sun公司的收购之后，Oracle公司分别从BEA和Sun中取得了目前三大商业虚拟机的其中两个：JRockit和HotSpot，Oracle公司宣布在未来1～2年的时间内，将把这两个优秀的虚拟机互相取长补短，最终合二为一。可以预见在不久的将来，Java虚拟机技术将会产生相当巨大的变化。 2011年7月28日，Oracle公司发布Java SE 7 2014年3月18日，Oracle公司发表Java SE 8 2017年9月22日，Oracle公司发表Java SE 9 JAVA虚拟机JAVA虚拟机是整个JAVA平台的基石，是JAVA技术用以实现硬件无关与操作系统无关的关键部分，是Java语言生成出极小体积的编译代码的运行平台，是保障用户机器免于恶意代码损害的屏障。 JAVA虚拟机可以看做是一台抽象的计算，有自己的指令集以及各种运行时内存区域。 JAVA虚拟机与JAVA语言并没有必然的联系，它只与特定的二进制文件格式–class文件格式所关联。class文件包含了JAVA虚拟机指令集和符号表，以及其他的一些辅助信息。 基于安全方面的考虑，Java虚拟机在class文件中施加了许多强制性的语法和结构化约束，凡是能用class文件正确表达出来的编程语言，都可以放在Java虚拟机里面执行。由于她是一个通用的、与机器无关的执行平台，所以其他语言的实现者都可以考虑将Java虚拟机作为那些语言的交付媒介。 JAVA学习目录官方学习教程：官方指南 JAVA虚拟机结构]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>虚拟机</tag>
        <tag>JVM</tag>
        <tag>JAVA8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何在window10下安装Gradle]]></title>
    <url>%2F2017%2F09%2F29%2FHow-to-install-gradle-on-windows-10%2F</url>
    <content type="text"><![CDATA[Gradle介绍Gradle是一个基于JVM的构建工具，它提供了： 像Ant一样，通用灵活的构建工具 可以切换的，基于约定的构建框架 强大的多工程构建支持 基于Apache Ivy的强大的依赖管理 支持maven, Ivy仓库 支持传递性依赖管理，而不需要远程仓库或者是pom.xml和ivy.xml配置文件。 对Ant的任务做了很好的集成 基于Groovy，build脚本使用Groovy编写 有广泛的领域模型支持构建 Gradle 概述 基于声明和基于约定的构建。 依赖型的编程语言。 可以结构化构建，易于维护和理解。 有高级的API允许你在构建执行的整个过程当中，对它的核心进行监视，或者是配置它的行为。 有良好的扩展性。有增量构建功能来克服性能瓶颈问题。 多项目构建的支持。 多种方式的依赖管理。 是第一个构建集成工具。集成了Ant, maven的功能。 易于移值。 脚本采用Groovy编写，易于维护。 通过Gradle Wrapper允许你在没有安装Gradle的机器上进行Gradle构建。 自由，开源。 Gradle 安装 JDK安装 推荐JDK8 Gradle官网 链接 下载地址链接 目前最新版本 V4.2 下载 gradle-4.2-all.zip，解压到自定义路径下。 配置环境变量。配置GRADLE_HOME到你的gradle根目录当中，然后把%GRADLE_HOME%/bin（linux或mac的是$GRADLE_HOME/bin）加到PATH的环境变量。 配置完成之后，运行gradle -v，检查一下是否安装无误。如果安装正确，它会打印出Gradle的版本信息，包括它的构建信息，Groovy, Ant, Ivy, 当前JVM和当前系统的版本信息。 Gradle仓库配置仓库类型 Maven central repository 这是Maven的中央仓库，无需配置，直接声明就可以使用。但不支持https协议访问 Maven JCenter repository JCenter中央仓库，实际也是是用的maven搭建的，但相比Maven仓库更友好，通过CDN分发，并且支持https访问。 Maven local repository Maven本地的仓库，可以通过本地配置文件进行配置 Maven repository 常规的第三方Maven仓库，可设置访问Url Ivy repository Ivy仓库，可以是本地仓库，也可以是远程仓库 Flat directory repository 使用本地文件夹作为仓库 使用方法 Maven central repository 在build.gradle中配置 123repositories &#123; mavenCentral()&#125; 就可以直接使用了。 Maven JCenter repository 123repositories &#123; jcenter()&#125; 这时使用jcenter仓库是通过https访问的，如果想切换成http协议访问，需要修改配置： 12345repositories &#123; jcenter &#123; url "http://jcenter.bintray.com" &#125;&#125; Local Maven repository可以使用Maven本地的仓库。默认情况下，本地仓库位于USER_HOME/.m2/repository（例如windows环境中，在C:\Users\NAME.m2.repository），同时可以通过USER_HOME/.m2/下的settings.xml配置文件修改默认路径位置。若使用本地仓库在build.gradle中进行如下配置： 123repositories &#123; mavenLocal()&#125; Maven repositories 第三方的配置也很简单，直接指明url即可： 12345repositories &#123; maven &#123; url "http://repo.mycompany.com/maven2" &#125;&#125; Ivy repository 配置如下： 12345repositories &#123; ivy &#123; url "http://repo.mycompany.com/repo" &#125;&#125; Flat directory repository 使用本地文件夹，这个也比较常用。直接在build.gradle中声明文件夹路径： 12345678repositories &#123; flatDir &#123; dirs 'lib' &#125; flatDir &#123; dirs 'lib1', 'lib2' &#125;&#125; 使用本地文件夹时，就不支持配置元数据格式的信息了（POM文件）。并且Gradle会优先使用服务器仓库中的库文件：例如同时声明了jcenter和flatDir，当flatDir中的库文件同样在jcenter中存在，gradle会优先使用jcenter的。 使用阿里镜像在 USER_HOME/.gradle/ 下面创建新文件 init.gradle，输入下面的内容并保存。1234567891011121314151617allprojects&#123; repositories &#123; def REPOSITORY_URL = &apos;http://maven.aliyun.com/nexus/content/groups/public/&apos; all &#123; ArtifactRepository repo -&gt; if(repo instanceof MavenArtifactRepository)&#123; def url = repo.url.toString() if (url.startsWith(&apos;https://repo1.maven.org/maven2&apos;) || url.startsWith(&apos;https://jcenter.bintray.com/&apos;)) &#123; project.logger.lifecycle &quot;Repository $&#123;repo.url&#125; replaced by $REPOSITORY_URL.&quot; remove repo &#125; &#125; &#125; maven &#123; url REPOSITORY_URL &#125; &#125;&#125; 修改本地仓库的位置设置Grdle环境变量,需要重启系统生效1RADLE_USER_HOME=d:/gradle-4.2/repo/]]></content>
      <categories>
        <category>Gradle</category>
      </categories>
      <tags>
        <tag>Gradle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 GitHub Pages and Hexo 搭建免费个人博客]]></title>
    <url>%2F2017%2F09%2F24%2FHow-to-build-a-personal-blog-using-GitHub-Pages-and-Hexo%2F</url>
    <content type="text"><![CDATA[Hexo介绍 Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 官网：Hexo Hexo安装环境安装 Node.js Git hexo运行所需环境具体安装不在本文讲解范围，不懂的朋友请通过百度或是google解决。 环境安装好，我们就可以通过npm完成hexo的安装。 1npm install -g hexo-cli GitHub创建博客仓库1.创建仓库 博客搭建的前提是你已经在Github上面注册账号，还没有的朋友，赶紧注册一个啦。官网：https://github.com/ 登录你的Github帐号，新建仓库，名字为【用户名.github.io】固定写法。 2.配置SSH-KEY 生成key,在默认位置（C:\Users\账户.ssh）生成id_rsa.pub和id_rsa两个文件。 1ssh-keygen -t rsa -C &quot;zhdevelop@gmail.com&quot; 打开公钥文件 id_rsa.pub ，并把内容复制至代码托管平台上 接下来把创建好的仓库克隆岛我们本地。 初始化Hexo先建立一个空文件夹blog，然后进入这个文件夹blog，输入如下命令 1mkdir blog &amp;&amp; cd blog 执行初始化命令1hexo init 然后把生成好的文件copy到我们从github克隆的分支目录里面。 Hexo初探安装hexo需要的运行环境在本地仓库目录下执行，会发现多了个node_modules文件夹，这里面都是npm相关包，是程序运行必须的环依赖。1npm install 生成发布内容博客生成需要执行如下命令1hexo g 执行完毕，我们会发现public文件夹里面生成了最后发布的文件。 本地预览终端执行如下1hexo s 打开http://localhos:4000进行本地预览。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>github</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
